#!/usr/bin/env python3

# Running mypy and black:
#     VENV=/tmp/venv
#     PYTHON=python3
#     mkdir -p $VENV
#     $PYTHON -m venv $VENV
#     . $VENV/bin/activate
#     pip install --upgrade pip mypy black
#     mypy ~/env/bin/borgadm
#     black ~/env/bin/borgadm
#     deactivate
#     rm -rf $VENV

from __future__ import annotations

import argparse
import atexit
import collections
import configparser
import enum
import fcntl
import functools
import io
import itertools
import json
import logging
import os
import platform
import re
import shlex
import shutil
import signal
import stat
import subprocess
import sys
import tempfile
import time
import typing
import unittest

from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Dict, Optional, OrderedDict, Tuple
from xml.dom import minidom
from xml.etree import ElementTree

# -----------------------------------------------------------------------------
# Constants and Globals
# -----------------------------------------------------------------------------
BASENAME: str = os.path.splitext(os.path.basename(__file__))[0]
LOGFILE: Path = Path(f"/tmp/{BASENAME}.log")
CONFIG: Path = Path(os.path.expanduser(f"~/.{BASENAME}"))

_borg_env_initialized: bool = False  # Ensures env is only initialized once
_enable_notifications: bool = False
_hostkey_file: typing.IO[str] = tempfile.NamedTemporaryFile(mode="w+")
_logger_buffer: io.StringIO
_ssh_agent_pid: Optional[str] = None
_start_time: float = time.perf_counter()

CFG: Config

logger = logging.getLogger()


# -----------------------------------------------------------------------------
# Configuration, optional
# -----------------------------------------------------------------------------
_BORG_PASSPHRASE_FILE: str = "~/.borg_passphrase"
_BORG_SSHKEY_FILE: str = "~/.ssh/id_borg.net"
_BORG_REMOTE_PATH: str = ""
_BORG_CMD_TIMEOUT: int = 4 * 3600  # Timeout for borg commands: 4 hours
_CMD_TIMEOUT: int = 60  # Timeout for non-borg commands: 1 minute

_BACKUP_NAME: str = "home"
_BACKUP_ROOT: str = "~"
_BACKUP_MOUNTS: list[str] = []

_CHECK_AGE_SECONDS: int = 24 * 3600  # Max backup age: 24 hours

_PRUNE_KEEP_HOURLY: int = 24
_PRUNE_KEEP_DAILY: int = 7
_PRUNE_KEEP_WEEKLY: int = 4
_PRUNE_KEEP_MONTHLY: int = 12
_PRUNE_KEEP_YEARLY: int = 2


# -----------------------------------------------------------------------------
# Configuration, required
# -----------------------------------------------------------------------------
_BORG_REPO: str = ""
_BORG_REPO_HOSTKEY: str = ""

# Format: { $set1_name: "create_options": ["opt1", ...], "paths": ["path1", ...] }
_BACKUP_SETS: dict[str, dict[str, list[str]]] = {}


# -----------------------------------------------------------------------------
# Classes
# -----------------------------------------------------------------------------
class Config:
    def __init__(self, path: str, args: dict[str, Any]) -> None:
        command = args["command"]
        path = path
        try:
            with open(path) as f:
                content = f.read()
        except (FileNotFoundError, PermissionError, IsADirectoryError, OSError) as e:
            logger.error(f"Failed to load config file {path}: {e}")
            sys.exit(1)

        config = configparser.ConfigParser()
        config_data = io.StringIO("[default]\n" + content)
        try:
            config.read_file(config_data)
        except configparser.ParsingError as e:
            logger.error(f"Failed to parse config file {path}: {e}")
            sys.exit(1)

        self.BORG_PASSPHRASE_FILE: Path = Path(
            os.path.expanduser(
                config.get(
                    "default", "BORG_PASSPHRASE_FILE", fallback=_BORG_PASSPHRASE_FILE
                )
            )
        )
        self.BORG_SSHKEY_FILE: Path = Path(
            os.path.expanduser(
                config.get("default", "BORG_SSHKEY_FILE", fallback=_BORG_SSHKEY_FILE)
            )
        )
        self.BORG_REMOTE_PATH: str = config.get(
            "default", "BORG_REMOTE_PATH", fallback=_BORG_REMOTE_PATH
        )
        self.BORG_CMD_TIMEOUT: int = config.getint(
            "default", "BORG_CMD_TIMEOUT", fallback=_BORG_CMD_TIMEOUT
        )
        self.CMD_TIMEOUT: int = config.getint(
            "default", "CMD_TIMEOUT", fallback=_CMD_TIMEOUT
        )

        self.BACKUP_NAME: str = config.get(
            "default", "BACKUP_NAME", fallback=_BACKUP_NAME
        )
        self.BACKUP_ROOT: Path = Path(
            os.path.expanduser(
                config.get("default", "BACKUP_ROOT", fallback=_BACKUP_ROOT)
            )
        )

        if (v := args.pop("seconds", None)) is not None:
            self.CHECK_AGE_SECONDS: int = v
        else:
            self.CHECK_AGE_SECONDS = config.getint(
                "default", "CHECK_AGE_SECONDS", fallback=_CHECK_AGE_SECONDS
            )

        if (v := args.pop("keep_hourly", None)) is not None:
            self.PRUNE_KEEP_HOURLY: int = v
        else:
            self.PRUNE_KEEP_HOURLY = config.getint(
                "default", "PRUNE_KEEP_HOURLY", fallback=_PRUNE_KEEP_HOURLY
            )
        if (v := args.pop("keep_daily", None)) is not None:
            self.PRUNE_KEEP_DAILY: int = v
        else:
            self.PRUNE_KEEP_DAILY = config.getint(
                "default", "PRUNE_KEEP_DAILY", fallback=_PRUNE_KEEP_DAILY
            )
        if (v := args.pop("keep_weekly", None)) is not None:
            self.PRUNE_KEEP_WEEKLY: int = v
        else:
            self.PRUNE_KEEP_WEEKLY = config.getint(
                "default", "PRUNE_KEEP_WEEKLY", fallback=_PRUNE_KEEP_WEEKLY
            )
        if (v := args.pop("keep_monthly", None)) is not None:
            self.PRUNE_KEEP_MONTHLY: int = v
        else:
            self.PRUNE_KEEP_MONTHLY = config.getint(
                "default", "PRUNE_KEEP_MONTHLY", fallback=_PRUNE_KEEP_MONTHLY
            )

        if (v := args.pop("keep_yearly", None)) is not None:
            self.PRUNE_KEEP_YEARLY: int = v
        else:
            self.PRUNE_KEEP_YEARLY = config.getint(
                "default", "PRUNE_KEEP_YEARLY", fallback=_PRUNE_KEEP_YEARLY
            )

        self.BORG_REPO: str = config.get("default", "BORG_REPO", fallback=_BORG_REPO)
        self.BORG_REPO_HOSTKEY: str = config.get(
            "default", "BORG_REPO_HOSTKEY", fallback=_BORG_REPO_HOSTKEY
        )
        self.BACKUP_MOUNTS: list[str] = json.loads(
            config.get("default", "BACKUP_MOUNTS", fallback=json.dumps(_BACKUP_MOUNTS))
        )
        self.BACKUP_SETS: dict[str, dict[str, list[str]]] = json.loads(
            config.get("default", "BACKUP_SETS", fallback=json.dumps(_BACKUP_SETS))
        )

        error = False
        if not self.BORG_REPO:
            logger.error(f"BORG_REPO not defined in config: {path}")
            error = True
        if not self.BACKUP_SETS:
            logger.error(f"BACKUP_SETS not defined in config: {path}")
            error = True
        if error:
            sys.exit(1)


class Interval(enum.Enum):
    HOUR = "hour"
    DAY = "day"
    WEEK = "week"
    MONTH = "month"
    YEAR = "year"

    def __str__(self) -> str:
        return self.value

    def timedelta(self) -> timedelta:
        if self == Interval.HOUR:
            return timedelta(hours=1)
        elif self == Interval.DAY:
            return timedelta(days=1)
        elif self == Interval.WEEK:
            return timedelta(weeks=1)
        elif self == Interval.MONTH:
            return timedelta(days=30)
        elif self == Interval.YEAR:
            return timedelta(days=365)
        else:
            raise ValueError(f"Unsupported timedelta interval: {self}")

    def keep(self) -> int:
        if self == Interval.HOUR:
            return CFG.PRUNE_KEEP_HOURLY
        elif self == Interval.DAY:
            return CFG.PRUNE_KEEP_DAILY
        elif self == Interval.WEEK:
            return CFG.PRUNE_KEEP_WEEKLY
        elif self == Interval.MONTH:
            return CFG.PRUNE_KEEP_MONTHLY
        elif self == Interval.YEAR:
            return CFG.PRUNE_KEEP_YEARLY
        else:
            raise ValueError(f"Unsupported keep interval: {self}")


# -----------------------------------------------------------------------------
# Utility Functions
# -----------------------------------------------------------------------------
def has_tty() -> bool:
    return sys.stdin.isatty() or sys.stdout.isatty() or sys.stderr.isatty()


def osascript_notify() -> None:
    global _enable_notifications, _logger_buffer
    if not _enable_notifications:
        return
    if not has_tty() and shutil.which("osascript"):
        logs = _logger_buffer.getvalue().strip().replace('"', '\\"')[:10000]
        script = f'display dialog "{logs}" with title "borg_backup.py error"'
        cmd = ["osascript", "-e", script]
        # No timeout is needed since we don't wait for the commmand to finish.
        subprocess.Popen(
            cmd,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            start_new_session=True,
        )


class InfoFilter(logging.Filter):
    def filter(self, record) -> bool:
        return record.levelno == logging.INFO


class WarnOrWorseFilter(logging.Filter):
    def __init__(self, verbose) -> None:
        self.verbose = verbose

    def filter(self, record) -> bool:
        if self.verbose:
            # skip INFO messages
            return record.levelno != logging.INFO
        return record.levelno >= logging.WARNING


def log_exceptions(exc_type, exc_value, exc_traceback) -> None:
    logger.critical("Uncaught exception", exc_info=(exc_type, exc_value, exc_traceback))
    osascript_notify()


def initialize_logger(logfile_path, verbose=False) -> None:
    logger.setLevel(logging.DEBUG)  # Capture all logs for the file

    # In memory logger for notifications
    global _logger_buffer
    _logger_buffer = io.StringIO()
    memory_handler = logging.StreamHandler(_logger_buffer)
    memory_handler.setLevel(logging.DEBUG if verbose else logging.WARNING)
    memory_handler.addFilter(WarnOrWorseFilter(verbose))
    memory_formatter = logging.Formatter(
        "%(asctime)s: PID %(process)d: %(levelname)s: %(message)s"
    )
    memory_handler.setFormatter(memory_formatter)
    logger.addHandler(memory_handler)

    # File handler - all messages with full formatting
    file_handler = logging.FileHandler(logfile_path)
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        "%(asctime)s: PID %(process)d: %(levelname)s: %(message)s"
    )
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)

    # Stdout handler for INFO messages only, unformatted
    stdout_handler = logging.StreamHandler(sys.stdout)
    stdout_handler.setLevel(logging.INFO)
    stdout_handler.addFilter(InfoFilter())
    stdout_handler.setFormatter(logging.Formatter("%(message)s"))
    logger.addHandler(stdout_handler)

    # Stderr handler for WARNING and worse (or DEBUG+ if verbose), formatted
    stderr_handler = logging.StreamHandler(sys.stderr)
    stderr_handler.setLevel(logging.DEBUG if verbose else logging.WARNING)
    stderr_handler.addFilter(WarnOrWorseFilter(verbose))
    stderr_formatter = logging.Formatter("%(levelname)s: %(message)s")
    stderr_handler.setFormatter(stderr_formatter)
    logger.addHandler(stderr_handler)

    sys.excepthook = log_exceptions


def find_borg_bin() -> None:
    if not shutil.which("borg"):
        path_search = ["/opt/local/bin"]
        for i in path_search:
            if os.path.exists(os.path.join(i, "borg")):
                os.environ["PATH"] = os.environ.get("PATH", "") + f":{i}"
                break
    if not shutil.which("borg"):
        logger.error(f"Can't find borg executable.")
        sys.exit(1)


def initialize_borg_environment() -> None:
    global _borg_env_initialized, _hostkey_file
    if _borg_env_initialized:
        return

    if CFG.BORG_REMOTE_PATH:
        os.environ["BORG_REMOTE_PATH"] = CFG.BORG_REMOTE_PATH
        logger.debug(f"BORG_REMOTE_PATH={os.environ['BORG_REMOTE_PATH']}")
    os.environ["BORG_PASSPHRASE"] = load_passphrase()
    _hostkey_file.write(CFG.BORG_REPO_HOSTKEY)
    _hostkey_file.flush()
    os.environ["BORG_RSH"] = (
        f"ssh -o StrictHostKeyChecking=yes -o UserKnownHostsFile={_hostkey_file.name}"
    )
    logger.debug(f"BORG_RSH={os.environ['BORG_RSH']}")
    start_ssh_agent()
    _borg_env_initialized = True


def check_permissions(path: Path, name: str) -> None:
    try:
        st = os.stat(path)
        if (st.st_mode & (stat.S_IRWXG | stat.S_IRWXO)) != 0:
            logger.error(f"Permissions on {name} ({path}) are too open.")
            sys.exit(4)
    except FileNotFoundError:
        logger.error(f"{name} not found at: {path}")
        sys.exit(4)


def run_cmd(
    cmd: list[str], sudo: bool = False, errok: bool = False, **kwargs
) -> subprocess.CompletedProcess[Any]:
    slow_cmds = ["borg", "rsync"]
    timeout: int = _BORG_CMD_TIMEOUT if cmd[0] in slow_cmds else _CMD_TIMEOUT
    if CFG:
        timeout = CFG.BORG_CMD_TIMEOUT if cmd[0] in slow_cmds else CFG.CMD_TIMEOUT
    if sudo:
        cmd = ["sudo", "-n", "-E"] + cmd
    cmd_str = shlex.join(cmd)
    logger.debug(f"Running command: {cmd_str}")
    result = subprocess.run(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        timeout=timeout,
        **kwargs,
    )

    if result.stderr:
        logger.debug(f"stderr:\n{result.stderr.strip()}")

    if result.returncode != 0:
        logger.error(
            f"Command `{cmd_str}` failed with exit code {result.returncode} and stderr:\n{result.stderr.strip()}"
        )
        # Raise same error as check=True would
        if not errok:
            raise subprocess.CalledProcessError(
                returncode=result.returncode,
                cmd=result.args,
                output=result.stdout,
                stderr=result.stderr,
            )

    return result


def start_ssh_agent() -> None:
    global _ssh_agent_pid
    logger.debug("Initializing SSH agent.")
    check_permissions(CFG.BORG_SSHKEY_FILE, "SSH private key")
    result = run_cmd(["ssh-agent", "-s"])
    for line in result.stdout.splitlines():
        line = line.split(";")[0].strip()
        if line.startswith(("SSH_AUTH_SOCK=", "SSH_AGENT_PID=")):
            key, value = line.split("=")
            os.environ[key] = value
            if key == "SSH_AGENT_PID":
                _ssh_agent_pid = value
    result = run_cmd(["ssh-add", "-q", CFG.BORG_SSHKEY_FILE.as_posix()])
    atexit.register(stop_ssh_agent)


def stop_ssh_agent() -> None:
    global _ssh_agent_pid
    if _ssh_agent_pid:
        logger.debug("Stopping SSH agent.")
        try:
            run_cmd(["kill", _ssh_agent_pid])
        except Exception as e:
            logger.warning(f"Failed to kill SSH agent: {e}")
        _ssh_agent_pid = None


def load_passphrase() -> str:
    check_permissions(CFG.BORG_PASSPHRASE_FILE, "BORG_PASSPHRASE file")
    with open(CFG.BORG_PASSPHRASE_FILE, "r") as f:
        return f.read().strip()


def create_backup(
    archive_name: str, extra_args: list[str], dirs: list[str], dry_run: bool
) -> None:
    archive = f"{CFG.BORG_REPO}::{archive_name}"
    logger.info(f"Create archive, start: {archive}{' (dry-run)' if dry_run else ''})")
    cmd = ["borg", "create"]
    if dry_run:
        cmd.append("--dry-run")
    else:
        cmd += ["--stats"]
    cmd += extra_args + [archive] + [os.path.expanduser(d) for d in dirs]
    run_cmd(cmd, cwd=CFG.BACKUP_ROOT)
    logger.info(f"Create archive, finish: {archive}{' (dry-run)' if dry_run else ''})")


@functools.cache
def list_backups_raw() -> subprocess.CompletedProcess[Any]:
    return run_cmd(
        [
            "borg",
            "list",
            "--bypass-lock",
            "--lock-wait",
            str(CFG.BORG_CMD_TIMEOUT),
            "--short",
            CFG.BORG_REPO,
        ]
    )


def list_backups(latest: bool = False, partial: bool = False) -> dict[str, list[str]]:
    result = list_backups_raw()

    timestamps_by_type: dict[str, set[str]] = {key: set() for key in CFG.BACKUP_SETS}
    backup_set_names = "|".join(CFG.BACKUP_SETS)
    ts_pattern = re.compile(
        f"{CFG.BACKUP_NAME}-({backup_set_names})-" + r"(\d{8}_\d{6})"
    )
    for line in result.stdout.strip().splitlines():
        match = ts_pattern.match(line.strip())
        if match:
            set_name, timestamp = match.groups()
            if set_name in timestamps_by_type:
                timestamps_by_type[set_name].add(timestamp)

    all_ts: set[str] = set().union(*timestamps_by_type.values())
    rv: dict[str, list[str]] = {}

    for ts in sorted(all_ts, reverse=True):

        def include() -> None:
            rv[ts] = [
                f"{CFG.BORG_REPO}::{CFG.BACKUP_NAME}-{set_name}-{ts}"
                for set_name in CFG.BACKUP_SETS
                if ts in timestamps_by_type[set_name]
            ]

        is_full = all(
            ts in timestamps_by_type[set_name] for set_name in CFG.BACKUP_SETS
        )
        if not partial and is_full:
            include()
        if partial and not is_full:
            include()
        if rv and latest:
            break

    return rv


def is_mountpoint(path: Path) -> bool:
    try:
        output = subprocess.check_output(["mount"], text=True)
        for line in output.splitlines():
            if f" on {path} " in line:
                return True
    except subprocess.CalledProcessError:
        pass
    return False


def check_sudo() -> bool:
    try:
        subprocess.run(
            ["sudo", "-n", "-E", "true"],
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            timeout=CFG.CMD_TIMEOUT,
        )
        return True
    except subprocess.CalledProcessError:
        return False
    except FileNotFoundError:
        # sudo not installed
        return False


def backup_set_paths(set_name: Optional[str] = None) -> Tuple[set[Path], set[Path]]:
    """
    Return sets of directories and files from BACKUP_SETS paths.
    Directories end in a "/", all other entries are considered files.
    """
    paths: set[str] = {
        k
        for i, j in CFG.BACKUP_SETS.items()
        if set_name is None or i == set_name
        for k in j["paths"]
    }
    dirs: set[Path] = {Path(i) for i in paths if i.endswith("/")}
    files: set[Path] = {Path(i) for i in paths if not i.endswith("/")}
    return (dirs, files)


def delete_paths(root: Path, relative_paths: set[Path], dry_run: bool = True) -> None:
    """
    Delete paths under root. Does not follow symlinks.
    """
    logger.info(f"Deleting {len(relative_paths)} paths in: {root}")
    for rel in sorted(relative_paths, key=lambda p: -len(p.parts)):  # deepest first
        abs_path = root / rel
        if abs_path.is_dir():
            logger.debug(f"Deleting directory: {abs_path}")
            if not dry_run:
                shutil.rmtree(abs_path)
        else:
            logger.debug(f"Deleting: {abs_path}")
            if not dry_run:
                abs_path.unlink()


def ts_to_keep(timestamps: set[str]) -> OrderedDict[str, str]:
    """
    Given a list of timestamps, return a ordered dictionary (sorted by timestamp) of
    which ones to keep, implementing a retention GFS (Grandfather-father-son) retention
    policy. This duplicates the retention policy implemented by borg itself. We do this
    to enable testing of the policy, and because we borg doesn't support grouping of
    backups into sets like we do.
    """
    if not timestamps:
        return collections.OrderedDict()
    ts_keep: OrderedDict[str, str] = collections.OrderedDict()

    def keep_by_interval(interval: Interval) -> None:
        nonlocal timestamps, ts_keep

        last: Optional[datetime] = None
        ts: Optional[str] = None
        ts_keep_new: OrderedDict[str, str] = collections.OrderedDict()
        for ts in sorted(timestamps - set(ts_keep.keys())):
            assert ts not in ts_keep
            dt: datetime = datetime.strptime(ts, "%Y%m%d_%H%M%S")
            if last is None or (dt - last) >= interval.timedelta():
                last = dt
                ts_keep_new[ts] = str(interval)
        ts_latest = ts

        i: int = 0
        for ts in sorted(ts_keep_new.keys(), reverse=True):
            ts_keep_new[ts] = ts_keep_new[ts] + f"-{i}"
            i += 1

        logger.debug(
            f"interval={interval}, keep={interval.keep()}, len={len(ts_keep_new)}, ts={ts}"
        )

        if (
            len(ts_keep_new) < interval.keep()
            and ts_latest is not None
            and ts_latest not in ts_keep_new
        ):
            ts_keep_new[ts_latest] = str(interval) + "-next"

        for k in list(ts_keep_new.keys())[-interval.keep() :]:
            ts_keep[k] = ts_keep_new[k]

    keep_by_interval(Interval.HOUR)
    keep_by_interval(Interval.DAY)
    keep_by_interval(Interval.WEEK)
    keep_by_interval(Interval.MONTH)
    keep_by_interval(Interval.YEAR)

    # Sort the results by timestamp
    ts_keep = collections.OrderedDict(
        [(k, ts_keep[k]) for k in sorted(ts_keep.keys(), reverse=True)]
    )
    return ts_keep


def report_runtime() -> None:
    global _start_time
    elapsed_time = time.perf_counter() - _start_time
    elapsed_time_str = str(timedelta(seconds=int(elapsed_time)))
    logger.debug(f"Execution time (hh:mm:ss): {elapsed_time_str}")


def create_plist_element(
    task: str, args: list[str], interval: int
) -> ElementTree.Element:

    plist = ElementTree.Element("plist", version="1.0")
    dict_elem = ElementTree.SubElement(plist, "dict")

    def add_kv(key, value):
        ElementTree.SubElement(dict_elem, "key").text = key
        ElementTree.SubElement(dict_elem, "string").text = value

    def add_bool(key, val: bool):
        ElementTree.SubElement(dict_elem, "key").text = key
        ElementTree.SubElement(dict_elem, "true" if val else "false")

    def add_int(key, val: int):
        ElementTree.SubElement(dict_elem, "key").text = key
        ElementTree.SubElement(dict_elem, "integer").text = str(val)

    add_kv("Label", f"local.borgadm.{task}")

    # ProgramArguments
    ElementTree.SubElement(dict_elem, "key").text = "ProgramArguments"
    arr = ElementTree.SubElement(dict_elem, "array")
    for part in args:
        ElementTree.SubElement(arr, "string").text = part

    # Log paths
    log_path = f"/tmp/borgadm.{task}.launchd.log"
    add_kv("StandardOutPath", log_path)
    add_kv("StandardErrorPath", log_path)

    add_int("StartInterval", interval)
    add_kv("ProcessType", "Interactive")
    add_bool("LowPriorityIO", False)
    add_int("Nice", -5)

    return plist


def create_plist_file(element: ElementTree.Element, path: Path) -> None:
    rough_string = ElementTree.tostring(element, encoding="utf-8")
    reparsed = minidom.parseString(rough_string)
    fd = os.open(path, os.O_WRONLY | os.O_CREAT | os.O_TRUNC, 0o644)
    with os.fdopen(fd, "w") as f:
        f.write('<?xml version="1.0" encoding="UTF-8"?>\n')
        f.write('<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN"\n')
        f.write(' "http://www.apple.com/DTDs/PropertyList-1.0.dtd">\n')
        f.write(reparsed.toprettyxml(indent="    "))


# -----------------------------------------------------------------------------
# Subcommands
# -----------------------------------------------------------------------------
def do_automate(enable: bool, disable: bool) -> None:
    if platform.system() != "Darwin":
        logger.error("automate subcommand is only supported on osx")
        sys.exit(1)

    if not shutil.which("plutil"):
        logger.error("plutil not found in PATH")
        sys.exit(1)

    if not shutil.which("launchctl"):
        logger.error("launchctl not found in PATH")
        sys.exit(1)

    common_args: list[str] = ["/opt/local/bin/python3", os.path.abspath(__file__)]
    tasks: dict[str, dict[str, Any]] = {
        "create": {"args": ["create"], "interval": (60 * 60)},
        "check_age": {
            "args": ["check-age", "--enable-notifications"],
            "interval": (60 * 60 * 4),
        },
        "check_all": {
            "args": ["check-all", "--enable-notifications"],
            "interval": (60 * 60 * 24 * 7),
        },
    }
    task_name = "local.borgadm.{task}"
    task_dir: Path = Path(os.environ["HOME"]) / "Library" / "LaunchAgents"
    task2path: dict[str, Path] = {
        task: task_dir / (task_name.format(task=task) + ".plist") for task in tasks
    }
    cmd: list[str] = []

    if not (enable or disable):
        tasks = run_cmd(["launchctl", "list"]).stdout.splitlines()
        for task in task2path:
            tname = task_name.format(task=task)
            pattern = r"[ \t]" + re.escape(tname) + r"$"
            cpattern = re.compile(pattern)
            if any(cpattern.search(i) for i in tasks):
                status = "enabled"
            else:
                status = "disabled"
            logger.info(f"{tname}: {status}")
        return

    # Always remove any existing configs
    for task, path in task2path.items():
        if not path.exists():
            continue
        logger.info(f"Removing {task} launchd config: {str(path)}")
        cmd = ["launchctl", "unload", str(path)]
        run_cmd(cmd, errok=True)
        path.unlink()

    if not enable:
        return

    for task, cfg in tasks.items():
        logger.info(f"Creating {task} launchd config: {str(path)}")
        task_args = common_args + cfg["args"]
        pe = create_plist_element(task, task_args, cfg["interval"])
        path = task2path[task]
        create_plist_file(pe, path)
        cmd = ["plutil", str(path)]
        run_cmd(cmd)
        cmd = ["launchctl", "load", str(path)]
        run_cmd(cmd)


def do_break_lock() -> None:
    logger.info(f"Breaking repo lock: {CFG.BORG_REPO}")
    run_cmd(
        [
            "borg",
            "break-lock",
            CFG.BORG_REPO,
        ]
    )


def do_check_age() -> None:
    seconds = CFG.CHECK_AGE_SECONDS
    backups = list_backups(latest=True)
    if not backups:
        logger.error("No full backups found.")
        sys.exit(2)
    assert len(backups.keys()) == 1, backups
    latest = list(backups.keys())[0]

    ts = datetime.strptime(latest, "%Y%m%d_%H%M%S")
    age = int((datetime.now() - ts).total_seconds())
    if age > seconds:
        logger.error(
            f"Latest full backup ({latest}) is too old: {age} seconds; limit is: {seconds} seconds"
        )
        sys.exit(3)

    logger.info(
        f"Latest full backup ({latest}) age is: {age} seconds; limit is: {seconds} seconds"
    )


def do_check_all() -> None:
    do_check_age()
    do_check_repo()
    do_check_archives()


def do_check_archives() -> None:
    backups = list_backups(latest=True)
    if not backups:
        logger.error("No full backups found.")
        sys.exit(1)
    assert len(backups.keys()) == 1
    archives = list(backups.values())[0]
    for archive in archives:
        logger.info(f"Checking archive metadata: {archive}")
        run_cmd(["borg", "check", archive])


def do_check_repo() -> None:
    logger.info(f"Checking repo metadata.")
    run_cmd(["borg", "check", CFG.BORG_REPO])


def do_compact() -> None:
    logger.info(f"Compacting repo: {CFG.BORG_REPO}")
    run_cmd(
        [
            "borg",
            "compact",
            CFG.BORG_REPO,
        ]
    )


def do_create(dry_run: bool, no_prune: bool) -> None:
    def backup_set_tests(set_name: str) -> None:
        error: bool = False
        dirs, files = backup_set_paths(set_name)
        for d in dirs:
            full_path = CFG.BACKUP_ROOT / d
            if not os.path.isdir(full_path):
                logger.error(f"Missing backup dir (set: '{set_name}'): {full_path}")
                error = True
            elif not os.listdir(full_path):
                logger.error(f"Empty backup dir (set: '{set_name}'): {full_path}")
                error = True
        for f in files:
            full_path = CFG.BACKUP_ROOT / f
            if os.path.isdir(full_path):
                logger.error(
                    f"Missing trailing slash for backup directory in config (set: '{set_name}'): {full_path}"
                )
                error = True
            if not os.path.exists(full_path):
                logger.error(f"Missing backup path (set: '{set_name}'): {full_path}")
                error = True
        if error:
            sys.exit(1)

    # Run all tests before backups to fail-fast in case of errors
    for set_name in CFG.BACKUP_SETS:
        backup_set_tests(set_name)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    for set_name, cfg in CFG.BACKUP_SETS.items():
        # Backups can take a while, so re-run tests before each backup
        backup_set_tests(set_name)
        for path in CFG.BACKUP_MOUNTS:
            full_path = CFG.BACKUP_ROOT / path
            if not is_mountpoint(full_path):
                logger.error(f"Missing backup mount: {full_path}")
                sys.exit(1)
        create_options = cfg.get("create_options", [])
        paths = cfg["paths"]
        create_backup(
            f"{CFG.BACKUP_NAME}-{set_name}-{timestamp}", create_options, paths, dry_run
        )

    if not no_prune:
        do_prune(dry_run=dry_run)


def do_extract(
    target_dir: Path, patterns: list[str], dry_run: bool, delete: bool
) -> None:
    backups = list_backups(latest=True)
    if not backups:
        logger.error("No full backups found.")
        sys.exit(1)
    assert len(backups.keys()) == 1
    archives = list(backups.values())[0]

    target_dir = target_dir.resolve()
    if not os.path.isdir(target_dir):
        logger.error(f"Invalid extraction dirctory: {target_dir}")
        sys.exit(1)

    logger.info(f"Extract target dirctory: {target_dir}")

    cleanup_paths: set[Path] = set()
    if delete:
        archive_paths: set[Path] = set()
        for archive in archives:
            logger.info(f"Getting archive contents: {archive}")
            cmd = ["borg", "list", "--format", "{path}{NL}"]
            for pattern in patterns:
                cmd += ["--pattern", pattern]
            cmd.append(archive)
            for i in run_cmd(cmd).stdout.splitlines():
                archive_paths.add(Path(i))

        logger.info(f"Getting target-path contents: {target_dir}")
        local_paths: set[Path] = set()
        for walk_root, walk_dirs, walk_files in os.walk(target_dir):
            root_path = Path(walk_root)
            for name in walk_files + walk_dirs:
                full_path = root_path / name
                rel_path = full_path.relative_to(target_dir)
                local_paths.add(rel_path)

        logger.info(
            "Filtering local contents against backup paths and archive contents"
        )
        bs_dirs, bs_files = backup_set_paths()

        def backup_set_filter(path: Path, set_name: Optional[str] = None) -> bool:
            if path in bs_files:
                return True
            for d in bs_dirs:
                if path.is_relative_to(d):
                    return True
            return False

        local_paths = set(filter(backup_set_filter, local_paths))
        extras = sorted(
            local_paths - archive_paths, key=lambda x: len(x.parts)
        )  # shallowest first
        for path in extras:
            if not any(path.relative_to(p) for p in cleanup_paths):
                cleanup_paths.add(path)

        logger.info(
            f"Found {len(cleanup_paths)} paths to delete after archive extraction."
        )

    lockfile = target_dir / f".{BASENAME}_target_dir.lock"
    with open(lockfile, "w") as lf:
        try:
            fcntl.flock(lf, fcntl.LOCK_EX | fcntl.LOCK_NB)
        except BlockingIOError:
            logger.error("Another extract process is already running for this path.")
            sys.exit(1)

        for archive in archives:
            logger.info(f"Extracting: {archive} to {target_dir}")

            cmd = ["borg", "extract"]
            if dry_run:
                cmd.append("--dry-run")
            for pattern in patterns:
                cmd += ["--pattern", pattern]
            cmd.append(archive)
            run_cmd(cmd, cwd=target_dir)

    if cleanup_paths:
        delete_paths(target_dir, cleanup_paths, dry_run=dry_run)


def do_list(
    latest: bool,
    full_names: bool,
    include_partial: bool,
    only_partial: bool,
    keep_tags: bool,
) -> None:
    full_backups: dict[str, list[str]] = {}
    partial_backups: dict[str, list[str]] = {}
    if not only_partial:
        full_backups = list_backups(latest=latest)
    if include_partial or only_partial:
        partial_backups = list_backups(latest=latest, partial=True)
    backups = full_backups | partial_backups

    ts_keep: OrderedDict[str, str] = collections.OrderedDict()
    if keep_tags:
        ts_keep = ts_to_keep(set(full_backups.keys()))

    def get_keep_tag(ts) -> str:
        if not keep_tags:
            return ""
        if ts not in ts_keep:
            tag = "prune"
        else:
            tag = ts_keep[ts]
        return f" ({tag})"

    for ts, archives in backups.items():
        keep_tag = get_keep_tag(ts)
        if full_names:
            for archive in archives:
                logger.info(f"{archive}{keep_tag}")
        else:
            logger.info(f"{ts}{keep_tag}")


def do_prune(dry_run: bool) -> None:
    partial_archives = [
        a.removeprefix(f"{CFG.BORG_REPO}::")
        for archives in list_backups(partial=True).values()
        for a in archives
    ]
    if partial_archives:
        logger.info(f"Pruning partial archives: {partial_archives}")
        cmd = ["borg", "delete"]
        if dry_run:
            cmd.append("--dry-run")
        else:
            cmd.append("--stats")
        cmd.append(CFG.BORG_REPO)
        cmd += partial_archives
        run_cmd(cmd)

    archives = list_backups()
    ts_keep = ts_to_keep(set(archives.keys()))
    ts_prune = archives.keys() - ts_keep.keys()
    prune_archives = [
        a.removeprefix(f"{CFG.BORG_REPO}::") for ts in ts_prune for a in archives[ts]
    ]
    logger.info(f"Keeping {len(ts_keep)} and pruning {len(ts_prune)} backups.")
    for k, v in ts_keep.items():
        logger.info(f"Keeping backup: {k} ({v})")
    pruning = [f"{i}" for i in sorted(ts_prune, reverse=True)]
    logger.info(f"Pruning backups: {', '.join(pruning)}")
    if prune_archives:
        cmd = ["borg", "delete"]
        if dry_run:
            cmd.append("--dry-run")
        else:
            cmd.append("--stats")
        cmd.append(CFG.BORG_REPO)
        cmd += prune_archives
        run_cmd(cmd)

    logger.info(f"Compacting {CFG.BORG_REPO}")
    run_cmd(
        [
            "borg",
            "compact",
            CFG.BORG_REPO,
        ]
    )


def do_rsync(target_dir: Path, dry_run: bool, delete: bool) -> None:
    if platform.system() != "Linux":
        logger.error("Rsync subcommand is only supported on linux")
        sys.exit(1)

    if not check_sudo():
        logger.error(
            "Rsync subcommand requires non-interractive sudo support with the ability to set environment variables."
        )
        sys.exit(1)

    backups = list_backups(latest=True)
    if not backups:
        logger.error("No full backups found.")
        sys.exit(1)
    assert len(backups.keys()) == 1
    ts = list(backups.keys())[0]
    archives = list(backups.values())[0]

    target_dir = target_dir.resolve()
    if not os.path.isdir(target_dir):
        logger.error(f"Invalid extraction dirctory: {target_dir}")
        sys.exit(1)

    logger.info(f"Rsync target dirctory: {target_dir}")
    lockfile = target_dir / f".{BASENAME}_target_dir.lock"
    tmpdir_prefix = f".{BASENAME}_rsync_{ts}_"
    with open(lockfile, "w") as lf, tempfile.TemporaryDirectory(
        dir=str(Path.home()),
        prefix=tmpdir_prefix,
    ) as tdir:
        tmpdir = Path(tdir)
        try:
            fcntl.flock(lf, fcntl.LOCK_EX | fcntl.LOCK_NB)
        except BlockingIOError:
            logger.error("Another extract process is already running for this path.")
            sys.exit(1)

        cleanup_mounts: list[Path] = []
        cleanup_dirs: list[Path] = []
        try:
            archive_mounts: list[Path] = []
            for archive in archives:
                archive_name = archive.split("::", 1)[1]
                mount_dir = tmpdir / archive_name

                os.makedirs(mount_dir)
                cmd = [
                    "borg",
                    "mount",
                    "-o",
                    "allow_other",
                    "--bypass-lock",
                    archive,
                    mount_dir.as_posix(),
                ]
                logger.info(f"Mounting {archive} to: {mount_dir}")
                run_cmd(cmd, sudo=True)
                cleanup_mounts.append(mount_dir)
                archive_mounts.append(mount_dir)

                if not os.listdir(mount_dir):
                    logger.error(f"Empty archive mount: {archive}")
                    sys.exit(1)

            lowerdir = ":".join([i.as_posix() for i in archive_mounts])
            upperdir = tmpdir / "upperdir"
            workdir = tmpdir / "workdir"
            merged = tmpdir / "merged"
            os.makedirs(upperdir)
            os.makedirs(workdir)
            os.makedirs(merged)
            cmd = [
                "mount",
                "-t",
                "overlay",
                "overlay",
                "-o",
                f"lowerdir={lowerdir},upperdir={upperdir},workdir={workdir}",
                merged.as_posix(),
            ]
            logger.info(f"Merging archives with overlayfs: {merged}")
            run_cmd(cmd, sudo=True)
            cleanup_mounts.append(merged)
            cleanup_dirs.append(workdir)
            if not os.listdir(merged):
                logger.error(f"Empty overlayfs mount: {merged}")
                sys.exit(1)

            logger.info(f"Starting rsync to: {target_dir}")
            cmd = [
                "rsync",
                "-a",
                "--stats",
                "--human-readable",
            ]
            if delete:
                cmd.append("--delete")
            if dry_run:
                cmd.append("--dry-run")
            cmd += [merged.as_posix() + "/", target_dir.as_posix() + "/"]
            run_cmd(cmd)
            logger.info(f"Finished rsync to: {target_dir}")
        finally:
            # Cleanup root owned resources
            for m in cleanup_mounts:
                run_cmd(["umount", "-f", m.as_posix()], sudo=True, errok=True)
            for d in cleanup_dirs:
                run_cmd(["rm", "-rf", d.as_posix()], sudo=True, errok=True)


def do_environment() -> None:
    logger.info(f"ssh-add -q {CFG.BORG_SSHKEY_FILE}")
    logger.info(f"export BORG_PASSPHRASE=$(cat {CFG.BORG_PASSPHRASE_FILE})")
    logger.info(f"export BORG_REPO={CFG.BORG_REPO}")
    if CFG.BORG_REMOTE_PATH:
        logger.info(f"export BORG_REMOTE_PATH={CFG.BORG_REMOTE_PATH}")


def do_unittest() -> None:

    config_file: typing.IO[str] = tempfile.NamedTemporaryFile(mode="w+")
    config_file.write(
        """
    BORG_REPO = "foobar"
    BACKUP_SETS = { "set1": ["foo"] }
    """
    )
    config_file.flush()

    global CFG
    CFG = Config(config_file.name, {"command": "unittest"})

    unittest.main(argv=["first-arg-is-ignored"])
    sys.exit(0)


# -----------------------------------------------------------------------------
# Argument Parsing
# -----------------------------------------------------------------------------
def args_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Borg backup manager",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Always set formatter_class so that we display default values.
    def add_parser(name: str, **kwargs) -> argparse.ArgumentParser:
        kwargs["formatter_class"] = argparse.ArgumentDefaultsHelpFormatter
        return subparsers.add_parser(name, **kwargs)

    def add_delete(parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "--delete",
            action="store_true",
            help="Delete files in the destination which don't exist in the backup",
        )

    def add_dry_run(parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "--dry-run", action="store_true", help="don't make any changes"
        )

    def add_prune_args(parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "--keep-hourly", type=int, help="Number of hourly backups to keep"
        )
        parser.add_argument(
            "--keep-daily", type=int, help="Number of daily backups to keep"
        )
        parser.add_argument(
            "--keep-weekly", type=int, help="Number of weekly backups to keep"
        )
        parser.add_argument(
            "--keep-monthly", type=int, help="Number of monthly backups to keep"
        )
        parser.add_argument(
            "--keep-yearly", type=int, help="Number of yearly backups to keep"
        )

    def add_seconds(parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "seconds", type=int, nargs="?", help="Maximum backup age, in seconds"
        )

    def add_target_dir(parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "target_dir", type=Path, help="Target directly for extracted backup"
        )

    automate = add_parser(
        "automate",
        help="Enable/Disable automated backups and checks",
    )
    enable_disable = automate.add_mutually_exclusive_group()
    enable_disable.add_argument(
        "--enable",
        action="store_true",
        dest="enable",
        help="Enable automated backup creation and checks",
    )
    enable_disable.add_argument(
        "--disable",
        action="store_true",
        dest="disable",
        help="Disable automated backup creation and checks",
    )

    add_parser("break-lock", help="Break repo lock")

    check_age = add_parser("check-age", help="Check full backup age")
    add_seconds(check_age)

    check_all = add_parser("check-all", help="Run all checks")
    add_seconds(check_all)

    add_parser("check-repo", help="Check repo metadata")
    add_parser("check-archives", help="Check archive metadata")

    add_parser("compact", help="Check full backup metadata")

    create = add_parser("create", help="Create full backup")
    create.add_argument("--no-prune", action="store_true", help="Skip backup pruning")
    add_dry_run(create)
    add_prune_args(create)

    extract = add_parser("extract", help="Extract latest full backup")
    add_target_dir(extract)
    extract.add_argument(
        "patterns",
        nargs="*",
        help="include/exclude paths matching PATTERN, see 'borg help patterns' for details",
    )
    add_delete(extract)
    add_dry_run(extract)

    list_cmd = add_parser("list", help="List backups")
    list_cmd.add_argument(
        "--latest", action="store_true", help="Only list latest complete backup set"
    )
    list_cmd.add_argument(
        "--full-names",
        action="store_true",
        help="List full names of backups (instead of just timestamps)",
    )
    list_cmd.add_argument(
        "--keep-tags",
        action="store_true",
        help="Include keep tags (indicating what would be kept and deleted during a prune)",
    )
    add_prune_args(list_cmd)
    group = list_cmd.add_mutually_exclusive_group()
    group.add_argument(
        "--include-partial", action="store_true", help="Include partial backups"
    )
    group.add_argument("--only-partial", action="store_true", help="Only list partial")

    prune = add_parser("prune", help="Prune partial and old backups")
    add_dry_run(prune)
    add_prune_args(prune)

    rsync = add_parser("rsync", help="Rsync latest archive")
    add_target_dir(rsync)
    add_delete(rsync)
    add_dry_run(rsync)

    add_parser("environment", help="Display environment settings to run borg cli")

    for subparser in subparsers.choices.values():
        subparser.add_argument(
            "--config", type=str, default=CONFIG, help="borgadm config file"
        )
        subparser.add_argument(
            "--enable-notifications",
            action="store_true",
            help="Enable system notifications for command failures (only supported on osx)",
        )
        subparser.add_argument(
            "--verbose", action="store_true", help="Enable verbose output"
        )

    return parser


# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------
def main() -> None:
    args = args_parser().parse_args()
    args_dict = vars(args).copy()
    config = args_dict.pop("config")
    verbose = args_dict.pop("verbose")
    global _enable_notifications
    _enable_notifications = args_dict.pop("enable_notifications")

    initialize_logger(LOGFILE, verbose)

    invocation = " ".join(shlex.quote(arg) for arg in sys.argv)
    logger.debug(f"Invocation: {invocation}")
    atexit.register(report_runtime)

    global CFG
    CFG = Config(config, args_dict)
    command = args_dict.pop("command")

    initialize_borg_environment()
    find_borg_bin()

    if command not in ["break-lock", "check-repo", "compact", "environment"]:
        logger.debug(f"BACKUP_SETS={CFG.BACKUP_SETS}")

    command_callbacks: dict[str, Callable[..., Any]] = {
        "automate": do_automate,
        "break-lock": do_break_lock,
        "check-age": do_check_age,
        "check-all": do_check_all,
        "check-archives": do_check_archives,
        "check-repo": do_check_repo,
        "compact": do_compact,
        "create": do_create,
        "environment": do_environment,
        "extract": do_extract,
        "list": do_list,
        "prune": do_prune,
        "rsync": do_rsync,
    }
    try:
        command_callbacks[command](**args_dict)
    except SystemExit as e:
        if e.code != 0:
            osascript_notify()
        raise


# -----------------------------------------------------------------------------
# Tests
# -----------------------------------------------------------------------------
class TestArgparseHelp(unittest.TestCase):
    def test_help_messages_present(self):
        parser = args_parser()

        for action in parser._actions:
            if isinstance(action, argparse._SubParsersAction):
                for subcmd_name, subparser in action.choices.items():
                    for sub_action in subparser._actions:
                        # Ignore default help option added by argparse
                        if isinstance(sub_action, argparse._HelpAction):
                            continue
                        self.assertTrue(
                            sub_action.help and sub_action.help.strip(),
                            msg=f"Missing help for argument(s) {sub_action.option_strings or sub_action.dest} in subcommand '{subcmd_name}'",
                        )


class TestTimestampPruning(unittest.TestCase):

    @classmethod
    def setUpClass(cls) -> None:
        # Set CFG values before tests run
        pass
        CFG.PRUNE_KEEP_HOURLY = 2
        CFG.PRUNE_KEEP_DAILY = 2
        CFG.PRUNE_KEEP_WEEKLY = 2
        CFG.PRUNE_KEEP_MONTHLY = 2
        CFG.PRUNE_KEEP_YEARLY = 2

    def test_empty_set(self) -> None:
        self.assertEqual(ts_to_keep(set()), collections.OrderedDict())

    def test_incremental_prune(self) -> None:
        start = datetime(2000, 1, 1, 0, 0, 0)
        ts_all: set[str] = set()
        for i in range(int(24 * 365 * 3.5)):
            ts = start + timedelta(hours=i)
            ts_all.add(ts.strftime("%Y%m%d_%H%M%S"))
            ts_keep = ts_to_keep(ts_all)
            ts_all = set(ts_keep)
        ts_keep_verify = collections.OrderedDict(
            [
                ("20030701_110000", "hour-0"),
                ("20030701_100000", "hour-1"),
                ("20030701_000000", "day-0"),
                ("20030630_000000", "day-1"),
                ("20030629_000000", "week-0"),
                ("20030623_000000", "month-0"),
                ("20030620_000000", "week-1"),
                ("20030521_000000", "month-1"),
                ("20021231_000000", "year-0"),
                ("20011231_000000", "year-1"),
            ]
        )
        self.assertTrue(ts_keep == ts_keep_verify, f"Incorrect ts_keep: {ts_keep}")

    def test_bulk_prune(self) -> None:
        start = datetime(2000, 1, 1, 0, 0, 0)
        ts_all = set(
            [
                (start + timedelta(hours=i)).strftime("%Y%m%d_%H%M%S")
                for i in range(int(24 * 365 * 3.5))
            ]
        )
        ts_keep = ts_to_keep(ts_all)
        ts_keep_verify = collections.OrderedDict(
            [
                ("20030701_110000", "hour-0"),
                ("20030701_100000", "hour-1"),
                ("20030701_000000", "day-0"),
                ("20030630_000000", "day-1"),
                ("20030628_000000", "week-0"),
                ("20030621_000000", "week-1"),
                ("20030614_000000", "month-0"),
                ("20030515_000000", "month-1"),
                ("20021231_000000", "year-0"),
                ("20011231_000000", "year-1"),
            ]
        )
        self.assertTrue(ts_keep == ts_keep_verify, f"Incorrect ts_keep: {ts_keep}")


if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "unittest":
        do_unittest()
    main()
