#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# ///
# This is human generated code that's been AI modified
#
# IMPORTANT: This script has tests in tests/test_borgadm.py
# Always run the `self-test` subcommand after making modifications.

from __future__ import annotations

import argparse
import atexit
import collections
import configparser
import enum
import fcntl
import functools
import io
import json
import logging
import os
import platform
import re
import shlex
import shutil
import signal
import stat
import subprocess
import sys
import tempfile
import threading
import time
import typing

# Handle broken pipes gracefully (e.g., when piping to `head`).
# Restore default SIGPIPE behavior so the process exits cleanly.
if hasattr(signal, "SIGPIPE"):
    signal.signal(signal.SIGPIPE, signal.SIG_DFL)

from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Optional, OrderedDict, Tuple, TextIO, cast

from xml.dom import minidom
from xml.etree import ElementTree

# -----------------------------------------------------------------------------
# Constants and Globals
# -----------------------------------------------------------------------------
BASENAME: str = os.path.splitext(os.path.basename(__file__))[0]
LOGFILE: Path = Path(f"/tmp/{BASENAME}.log")
CONFIG: Path = Path(os.path.expanduser(f"~/.{BASENAME}"))

_borg_env_initialized: bool = False  # Ensures env is only initialized once
_enable_notifications: bool = False
_hostkey_file: typing.IO[str] = tempfile.NamedTemporaryFile(mode="w+")
_logger_buffer: io.StringIO
_ssh_agent_pid: Optional[str] = None
_start_time: float = time.perf_counter()

CFG: Optional[Config] = None

logger = logging.getLogger()


# -----------------------------------------------------------------------------
# Configuration, optional
# -----------------------------------------------------------------------------
_BORG_PASSPHRASE_FILE: str = "~/.borg_passphrase"
_BORG_SSHKEY_FILE: str = "~/.ssh/id_borg.net"
_BORG_REMOTE_PATH: str = ""
_BORG_CMD_TIMEOUT: int = 4 * 3600  # Timeout for borg commands: 4 hours
_CMD_TIMEOUT: int = 60  # Timeout for non-borg commands: 1 minute

_BACKUP_NAME: str = "home"
_BACKUP_ROOT: str = "~"
_BACKUP_MOUNTS: list[str] = []

_CHECK_AGE_SECONDS: int = 24 * 3600  # Max backup age: 24 hours

_PRUNE_KEEP_HOURLY: int = 24
_PRUNE_KEEP_DAILY: int = 7
_PRUNE_KEEP_WEEKLY: int = 4
_PRUNE_KEEP_MONTHLY: int = 12
_PRUNE_KEEP_YEARLY: int = 2

# macOS TCC-protected directories that cannot be accessed even with Full Disk
# Access. These require Apple-private entitlements only available to Apple-
# signed apps. Auto-excluded on Darwin to prevent backup errors.
_MACOS_TCC_EXCLUDES: list[str] = [
    # Application Support - TCC protected
    "Library/Application Support/AddressBook",
    "Library/Application Support/CallHistoryDB",
    "Library/Application Support/CallHistoryTransactions",
    "Library/Application Support/com.apple.TCC",
    "Library/Application Support/com.apple.avfoundation",
    "Library/Application Support/com.apple.sharedfilelist",
    "Library/Application Support/DifferentialPrivacy",
    "Library/Application Support/FaceTime",
    "Library/Application Support/Knowledge",
    # Library top-level - TCC protected
    "Library/Accounts",
    "Library/AppleMediaServices",
    "Library/Assistant/SiriVocabulary",
    "Library/Autosave Information",
    "Library/Biome",
    "Library/Calendars",
    "Library/com.apple.aiml.instrumentation",
    "Library/com.apple.bluetooth.services.cloud",
    "Library/ContainerManager",
    "Library/Cookies",
    "Library/CoreFollowUp",
    "Library/DoNotDisturb",
    "Library/DuetExpertCenter",
    "Library/HomeKit",
    "Library/IdentityServices",
    "Library/IntelligencePlatform",
    "Library/Mail",
    "Library/Messages",
    "Library/PersonalizationPortrait",
    "Library/Reminders",
    "Library/Safari",
    "Library/Sharing",
    "Library/Shortcuts",
    "Library/StatusKit",
    "Library/Suggestions",
    "Library/Trial",
    "Library/Weather",
    # Preferences - specific TCC-protected plists
    "Library/Preferences/com.apple.AddressBook.plist",
    "Library/Preferences/com.apple.homed.notbackedup.plist",
    "Library/Preferences/com.apple.homed.plist",
    "Library/Preferences/com.apple.mail-shared.plist",
    "Library/Preferences/com.apple.messages.pinning.plist",
    "Library/Preferences/com.apple.MobileSMS.CKDNDList.plist",
    "Library/Preferences/com.apple.MobileSMS.plist",
    # iCloud - causes deadlocks during sync
    "Library/Mobile Documents",
    # Other permission issues
    "Library/Google/GoogleSoftwareUpdate",
]


# -----------------------------------------------------------------------------
# Configuration, required
# -----------------------------------------------------------------------------
_BORG_REPO: str = ""
_BORG_REPO_HOSTKEY: str = ""

# Format:
# BACKUP_SETS = {
#   $set1_name: {
#       "create_options": ["opt1", ...],
#       "excludes": ["pattern1", ...], # optional
#       "paths": ["path1", ...]
#   }
# }
_BACKUP_SETS: dict[str, dict[str, list[str]]] = {}


# -----------------------------------------------------------------------------
# Classes
# -----------------------------------------------------------------------------
class Config:
    def __init__(self, path: str, args: dict[str, Any]) -> None:
        try:
            with open(path) as f:
                content = f.read()
        except (
            FileNotFoundError,
            PermissionError,
            IsADirectoryError,
            OSError,
        ) as e:
            logger.error(f"Failed to load config file {path}: {e}")
            sys.exit(1)

        config = configparser.ConfigParser()
        config_data = io.StringIO("[default]\n" + content)
        try:
            config.read_file(config_data)
        except configparser.ParsingError as e:
            logger.error(f"Failed to parse config file {path}: {e}")
            sys.exit(1)

        self.BORG_PASSPHRASE_FILE: Path = Path(
            os.path.expanduser(
                config.get(
                    "default",
                    "BORG_PASSPHRASE_FILE",
                    fallback=_BORG_PASSPHRASE_FILE,
                )
            )
        )
        self.BORG_SSHKEY_FILE: Path = Path(
            os.path.expanduser(
                config.get(
                    "default", "BORG_SSHKEY_FILE", fallback=_BORG_SSHKEY_FILE
                )
            )
        )
        self.BORG_REMOTE_PATH: str = config.get(
            "default", "BORG_REMOTE_PATH", fallback=_BORG_REMOTE_PATH
        )
        self.BORG_CMD_TIMEOUT: int = config.getint(
            "default", "BORG_CMD_TIMEOUT", fallback=_BORG_CMD_TIMEOUT
        )
        self.CMD_TIMEOUT: int = config.getint(
            "default", "CMD_TIMEOUT", fallback=_CMD_TIMEOUT
        )

        self.BACKUP_NAME: str = config.get(
            "default", "BACKUP_NAME", fallback=_BACKUP_NAME
        )
        self.BACKUP_ROOT: Path = Path(
            os.path.expanduser(
                config.get("default", "BACKUP_ROOT", fallback=_BACKUP_ROOT)
            )
        )

        if (v := args.pop("seconds", None)) is not None:
            self.CHECK_AGE_SECONDS: int = v
        else:
            self.CHECK_AGE_SECONDS = config.getint(
                "default", "CHECK_AGE_SECONDS", fallback=_CHECK_AGE_SECONDS
            )

        if (v := args.pop("keep_hourly", None)) is not None:
            self.PRUNE_KEEP_HOURLY: int = v
        else:
            self.PRUNE_KEEP_HOURLY = config.getint(
                "default", "PRUNE_KEEP_HOURLY", fallback=_PRUNE_KEEP_HOURLY
            )
        if (v := args.pop("keep_daily", None)) is not None:
            self.PRUNE_KEEP_DAILY: int = v
        else:
            self.PRUNE_KEEP_DAILY = config.getint(
                "default", "PRUNE_KEEP_DAILY", fallback=_PRUNE_KEEP_DAILY
            )
        if (v := args.pop("keep_weekly", None)) is not None:
            self.PRUNE_KEEP_WEEKLY: int = v
        else:
            self.PRUNE_KEEP_WEEKLY = config.getint(
                "default", "PRUNE_KEEP_WEEKLY", fallback=_PRUNE_KEEP_WEEKLY
            )
        if (v := args.pop("keep_monthly", None)) is not None:
            self.PRUNE_KEEP_MONTHLY: int = v
        else:
            self.PRUNE_KEEP_MONTHLY = config.getint(
                "default", "PRUNE_KEEP_MONTHLY", fallback=_PRUNE_KEEP_MONTHLY
            )

        if (v := args.pop("keep_yearly", None)) is not None:
            self.PRUNE_KEEP_YEARLY: int = v
        else:
            self.PRUNE_KEEP_YEARLY = config.getint(
                "default", "PRUNE_KEEP_YEARLY", fallback=_PRUNE_KEEP_YEARLY
            )

        self.BORG_REPO: str = config.get(
            "default", "BORG_REPO", fallback=_BORG_REPO
        )
        self.BORG_REPO_HOSTKEY: str = config.get(
            "default", "BORG_REPO_HOSTKEY", fallback=_BORG_REPO_HOSTKEY
        )
        self.BACKUP_MOUNTS: list[str] = json.loads(
            config.get(
                "default", "BACKUP_MOUNTS", fallback=json.dumps(_BACKUP_MOUNTS)
            )
        )
        self.BACKUP_SETS: dict[str, dict[str, list[str]]] = json.loads(
            config.get(
                "default", "BACKUP_SETS", fallback=json.dumps(_BACKUP_SETS)
            )
        )

        error = False
        if not self.BORG_REPO:
            logger.error(f"BORG_REPO not defined in config: {path}")
            error = True
        if not self.BACKUP_SETS:
            logger.error(f"BACKUP_SETS not defined in config: {path}")
            error = True
        if error:
            sys.exit(1)


class Interval(enum.Enum):
    HOUR = "hour"
    DAY = "day"
    WEEK = "week"
    MONTH = "month"
    YEAR = "year"

    def __str__(self) -> str:
        return self.value

    def timedelta(self) -> timedelta:
        if self == Interval.HOUR:
            return timedelta(hours=1)
        elif self == Interval.DAY:
            return timedelta(days=1)
        elif self == Interval.WEEK:
            return timedelta(weeks=1)
        elif self == Interval.MONTH:
            return timedelta(days=30)
        elif self == Interval.YEAR:
            return timedelta(days=365)
        else:
            raise ValueError(f"Unsupported timedelta interval: {self}")

    def keep(self) -> int:
        cfg = get_cfg()
        if self == Interval.HOUR:
            return cfg.PRUNE_KEEP_HOURLY
        elif self == Interval.DAY:
            return cfg.PRUNE_KEEP_DAILY
        elif self == Interval.WEEK:
            return cfg.PRUNE_KEEP_WEEKLY
        elif self == Interval.MONTH:
            return cfg.PRUNE_KEEP_MONTHLY
        elif self == Interval.YEAR:
            return cfg.PRUNE_KEEP_YEARLY
        else:
            raise ValueError(f"Unsupported keep interval: {self}")


# -----------------------------------------------------------------------------
# Utility Functions
# -----------------------------------------------------------------------------
def get_cfg() -> Config:
    assert CFG is not None, "CFG not initialized"
    return CFG


def has_tty() -> bool:
    return sys.stdin.isatty() or sys.stdout.isatty() or sys.stderr.isatty()


def osascript_notify() -> None:
    if not _enable_notifications:
        return
    if not has_tty() and shutil.which("osascript"):
        logs = _logger_buffer.getvalue().strip().replace('"', '\\"')[:10000]
        script = f'display dialog "{logs}" with title "borg_backup.py error"'
        cmd = ["osascript", "-e", script]
        # No timeout is needed since we don't wait for the commmand to finish.
        subprocess.Popen(
            cmd,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            start_new_session=True,
        )


class InfoFilter(logging.Filter):
    def filter(self, record) -> bool:
        return record.levelno == logging.INFO


class WarnOrWorseFilter(logging.Filter):
    def __init__(self, verbose) -> None:
        self.verbose = verbose

    def filter(self, record) -> bool:
        if self.verbose:
            # skip INFO messages
            return record.levelno != logging.INFO
        return record.levelno >= logging.WARNING


def log_exceptions(exc_type, exc_value, exc_traceback) -> None:
    logger.critical(
        "Uncaught exception", exc_info=(exc_type, exc_value, exc_traceback)
    )
    osascript_notify()


def initialize_logger(logfile_path, verbose=False) -> None:
    logger.setLevel(logging.DEBUG)  # Capture all logs for the file

    # In memory logger for notifications
    global _logger_buffer
    _logger_buffer = io.StringIO()
    memory_handler = logging.StreamHandler(_logger_buffer)
    memory_handler.setLevel(logging.DEBUG if verbose else logging.WARNING)
    memory_handler.addFilter(WarnOrWorseFilter(verbose))
    memory_formatter = logging.Formatter(
        "%(asctime)s: PID %(process)d: %(levelname)s: %(message)s"
    )
    memory_handler.setFormatter(memory_formatter)
    logger.addHandler(memory_handler)

    # File handler - all messages with full formatting
    file_handler = logging.FileHandler(logfile_path)
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        "%(asctime)s: PID %(process)d: %(levelname)s: %(message)s"
    )
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)

    # Stdout handler for INFO messages only, unformatted
    stdout_handler = logging.StreamHandler(sys.stdout)
    stdout_handler.setLevel(logging.INFO)
    stdout_handler.addFilter(InfoFilter())
    stdout_handler.setFormatter(logging.Formatter("%(message)s"))
    logger.addHandler(stdout_handler)

    # Stderr handler for WARNING and worse (or DEBUG+ if verbose), formatted
    stderr_handler = logging.StreamHandler(sys.stderr)
    stderr_handler.setLevel(logging.DEBUG if verbose else logging.WARNING)
    stderr_handler.addFilter(WarnOrWorseFilter(verbose))
    stderr_formatter = logging.Formatter("%(levelname)s: %(message)s")
    stderr_handler.setFormatter(stderr_formatter)
    logger.addHandler(stderr_handler)

    sys.excepthook = log_exceptions


@functools.cache
def borg_cmd() -> list[str]:

    # Default to the version in PATH.
    if rv := shutil.which("borg"):
        return [rv]

    # Explicitly search common dirs.
    path_search = [os.environ["HOME"] + "/.local/bin", "/opt/local/bin"]
    for i in path_search:
        rv = os.path.join(i, "borg")
        if os.path.exists(rv):
            return [rv]

    # Run it via uvx if it's installed.
    if uvx := shutil.which("uvx"):
        return [uvx, "-q", "--from", "borgbackup", "borg"]

    logger.error("Can't find borg or uvx executables.")
    sys.exit(1)


def initialize_borg_environment() -> None:
    cfg = get_cfg()

    global _borg_env_initialized
    if _borg_env_initialized:
        return

    if cfg.BORG_REMOTE_PATH:
        os.environ["BORG_REMOTE_PATH"] = cfg.BORG_REMOTE_PATH
        logger.debug(f"BORG_REMOTE_PATH={os.environ['BORG_REMOTE_PATH']}")
    os.environ["BORG_PASSPHRASE"] = load_passphrase()
    _hostkey_file.write(cfg.BORG_REPO_HOSTKEY)
    _hostkey_file.flush()
    # Use -F /dev/null to ignore user's ~/.ssh/config which may contain
    # settings (like IdentityAgent) that conflict with our ssh-agent setup.
    os.environ["BORG_RSH"] = (
        f"ssh -F /dev/null "
        f"-o StrictHostKeyChecking=yes "
        f"-o UserKnownHostsFile={_hostkey_file.name}"
    )
    logger.debug(f"BORG_RSH={os.environ['BORG_RSH']}")
    start_ssh_agent()
    _borg_env_initialized = True


def check_permissions(path: Path, name: str) -> None:
    try:
        st = os.stat(path)
        if (st.st_mode & (stat.S_IRWXG | stat.S_IRWXO)) != 0:
            logger.error(f"Permissions on {name} ({path}) are too open.")
            sys.exit(4)
    except FileNotFoundError:
        logger.error(f"{name} not found at: {path}")
        sys.exit(4)


def run_cmd(
    cmd: list[str],
    sudo: bool = False,
    errok: bool = False,
    allow_output: bool = False,
    **kwargs,
) -> subprocess.CompletedProcess[Any]:
    slow_cmds = borg_cmd() + ["rsync"]
    timeout: int = _BORG_CMD_TIMEOUT if cmd[0] in slow_cmds else _CMD_TIMEOUT
    if CFG:
        timeout = (
            CFG.BORG_CMD_TIMEOUT if cmd[0] in slow_cmds else CFG.CMD_TIMEOUT
        )
    if sudo:
        cmd = ["sudo", "-n", "-E"] + cmd
    cmd_str = shlex.join(cmd)
    logger.debug(f"Running command: {cmd_str}")
    proc = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        bufsize=1,
        **kwargs,
    )
    stdout_buf = io.StringIO()
    stderr_buf = io.StringIO()
    stdout = cast(TextIO, proc.stdout)
    stderr = cast(TextIO, proc.stderr)

    def pump(src: TextIO, sink: TextIO, acc: io.StringIO) -> None:
        while True:
            chunk = src.readline()
            if chunk == "":
                break
            acc.write(chunk)
            if allow_output:
                sink.write(chunk)
                sink.flush()

    t_stdout = threading.Thread(
        target=pump, args=(stdout, sys.stdout, stdout_buf), daemon=True
    )
    t_stdout.start()
    t_stderr = threading.Thread(
        target=pump, args=(stderr, sys.stderr, stderr_buf), daemon=True
    )
    t_stderr.start()

    try:
        proc.wait(timeout=timeout)
    except subprocess.TimeoutExpired:
        proc.kill()
        proc.wait()

    t_stdout.join()
    t_stderr.join()

    stdout_str = stdout_buf.getvalue()
    stderr_str = stderr_buf.getvalue()

    if stderr_str:
        logger.debug(f"stderr:\n{stderr_str.strip()}")

    if proc.returncode != 0:
        logger.error(
            "Command %s failed with exit code %s and stderr:\n%s",
            cmd_str,
            proc.returncode,
            stderr_str.strip(),
        )
        # Raise same error as check=True would
        if not errok:
            raise subprocess.CalledProcessError(
                returncode=proc.returncode,
                cmd=proc.args,
                output=stdout_str,
                stderr=stderr_str,
            )

    return subprocess.CompletedProcess(
        args=cmd,
        returncode=proc.returncode,
        stdout=stdout_str,
        stderr=stderr_str,
    )


def start_ssh_agent() -> None:
    cfg = get_cfg()

    global _ssh_agent_pid
    logger.debug("Initializing SSH agent.")
    check_permissions(cfg.BORG_SSHKEY_FILE, "SSH private key")
    result = run_cmd(["ssh-agent", "-s"])
    for line in result.stdout.splitlines():
        line = line.split(";")[0].strip()
        if line.startswith(("SSH_AUTH_SOCK=", "SSH_AGENT_PID=")):
            key, value = line.split("=")
            os.environ[key] = value
            if key == "SSH_AGENT_PID":
                _ssh_agent_pid = value
    result = run_cmd(["ssh-add", "-q", cfg.BORG_SSHKEY_FILE.as_posix()])
    atexit.register(stop_ssh_agent)


def stop_ssh_agent() -> None:
    global _ssh_agent_pid
    if _ssh_agent_pid:
        logger.debug("Stopping SSH agent.")
        try:
            run_cmd(["kill", _ssh_agent_pid])
        except Exception as e:
            logger.warning(f"Failed to kill SSH agent: {e}")
        _ssh_agent_pid = None


def load_passphrase() -> str:
    cfg = get_cfg()

    check_permissions(cfg.BORG_PASSPHRASE_FILE, "BORG_PASSPHRASE file")
    with open(cfg.BORG_PASSPHRASE_FILE, "r") as f:
        return f.read().strip()


@functools.cache
def list_backups_raw() -> subprocess.CompletedProcess[Any]:
    cfg = get_cfg()

    return run_cmd(
        borg_cmd()
        + [
            "list",
            "--bypass-lock",
            "--lock-wait",
            str(cfg.BORG_CMD_TIMEOUT),
            "--short",
            cfg.BORG_REPO,
        ]
    )


def list_backups(
    latest: bool = False, partial: bool = False
) -> dict[str, list[str]]:
    cfg = get_cfg()

    result = list_backups_raw()

    timestamps_by_type: dict[str, set[str]] = {
        key: set() for key in cfg.BACKUP_SETS
    }
    backup_set_names = "|".join(cfg.BACKUP_SETS)
    ts_pattern = re.compile(
        f"{cfg.BACKUP_NAME}-({backup_set_names})-" + r"(\d{8}_\d{6})"
    )
    for line in result.stdout.strip().splitlines():
        match = ts_pattern.match(line.strip())
        if match:
            set_name, timestamp = match.groups()
            if set_name in timestamps_by_type:
                timestamps_by_type[set_name].add(timestamp)

    all_ts: set[str] = set().union(*timestamps_by_type.values())
    rv: dict[str, list[str]] = {}

    for ts in sorted(all_ts, reverse=True):

        def include() -> None:
            rv[ts] = [
                f"{cfg.BORG_REPO}::{cfg.BACKUP_NAME}-{set_name}-{ts}"
                for set_name in cfg.BACKUP_SETS
                if ts in timestamps_by_type[set_name]
            ]

        is_full = all(
            ts in timestamps_by_type[set_name] for set_name in cfg.BACKUP_SETS
        )
        if not partial and is_full:
            include()
        if partial and not is_full:
            include()
        if rv and latest:
            break

    return rv


def is_mountpoint(path: Path) -> bool:
    try:
        output = subprocess.check_output(["mount"], text=True)
        for line in output.splitlines():
            if f" on {path} " in line:
                return True
    except subprocess.CalledProcessError:
        pass
    return False


def check_sudo() -> bool:
    cfg = get_cfg()

    try:
        subprocess.run(
            ["sudo", "-n", "-E", "true"],
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            timeout=cfg.CMD_TIMEOUT,
        )
        return True
    except subprocess.CalledProcessError:
        return False
    except FileNotFoundError:
        # sudo not installed
        return False


def backup_set_paths(
    set_name: Optional[str] = None,
) -> Tuple[set[Path], set[Path]]:
    """
    Return sets of directories and files from BACKUP_SETS paths.
    Directories end in a "/", all other entries are considered files.
    """
    cfg = get_cfg()

    paths: set[str] = {
        k
        for i, j in cfg.BACKUP_SETS.items()
        if set_name is None or i == set_name
        for k in j["paths"]
    }
    dirs: set[Path] = {Path(i) for i in paths if i.endswith("/")}
    files: set[Path] = {Path(i) for i in paths if not i.endswith("/")}
    return (dirs, files)


def delete_paths(
    root: Path, relative_paths: set[Path], dry_run: bool = True
) -> None:
    """
    Delete paths under root. Does not follow symlinks.
    """
    logger.info(f"Deleting {len(relative_paths)} paths in: {root}")
    for rel in sorted(
        relative_paths, key=lambda p: -len(p.parts)
    ):  # deepest first
        abs_path = root / rel
        if abs_path.is_dir():
            logger.debug(f"Deleting directory: {abs_path}")
            if not dry_run:
                shutil.rmtree(abs_path)
        else:
            logger.debug(f"Deleting: {abs_path}")
            if not dry_run:
                abs_path.unlink()


def ts_to_keep(timestamps: set[str]) -> OrderedDict[str, str]:
    """
    Given a list of timestamps, return a ordered dictionary (sorted by
    timestamp) of which ones to keep, implementing a retention GFS
    (Grandfather-father-son) retention policy. This duplicates the retention
    policy implemented by borg itself. We do this to enable testing of the
    policy, and because we borg doesn't support grouping of backups into
    sets like we do.
    """
    if not timestamps:
        return collections.OrderedDict()
    ts_keep: OrderedDict[str, str] = collections.OrderedDict()

    def keep_by_interval(interval: Interval) -> None:
        last: Optional[datetime] = None
        ts: Optional[str] = None
        ts_keep_new: OrderedDict[str, str] = collections.OrderedDict()
        for ts in sorted(timestamps - set(ts_keep.keys())):
            assert ts not in ts_keep
            dt: datetime = datetime.strptime(ts, "%Y%m%d_%H%M%S")
            if last is None or (dt - last) >= interval.timedelta():
                last = dt
                ts_keep_new[ts] = str(interval)
        ts_latest = ts

        i: int = 0
        for ts in sorted(ts_keep_new.keys(), reverse=True):
            ts_keep_new[ts] = ts_keep_new[ts] + f"-{i}"
            i += 1

        logger.debug(
            "interval=%s, keep=%s, len=%s, ts=%s",
            interval,
            interval.keep(),
            len(ts_keep_new),
            ts,
        )

        if (
            len(ts_keep_new) < interval.keep()
            and ts_latest is not None
            and ts_latest not in ts_keep_new
        ):
            ts_keep_new[ts_latest] = str(interval) + "-next"

        for k in list(ts_keep_new.keys())[-interval.keep() :]:
            ts_keep[k] = ts_keep_new[k]

    keep_by_interval(Interval.HOUR)
    keep_by_interval(Interval.DAY)
    keep_by_interval(Interval.WEEK)
    keep_by_interval(Interval.MONTH)
    keep_by_interval(Interval.YEAR)

    # Sort the results by timestamp
    ts_keep = collections.OrderedDict(
        [(k, ts_keep[k]) for k in sorted(ts_keep.keys(), reverse=True)]
    )
    return ts_keep


def report_runtime() -> None:
    elapsed_time = time.perf_counter() - _start_time
    elapsed_time_str = str(timedelta(seconds=int(elapsed_time)))
    logger.debug(f"Execution time (hh:mm:ss): {elapsed_time_str}")


def create_plist_element(
    task: str,
    args: list[str],
    interval: int,
    env: dict[str, str],
    log_path: str,
) -> ElementTree.Element:
    plist = ElementTree.Element("plist", version="1.0")
    dict_elem = ElementTree.SubElement(plist, "dict")

    def add_kv(key, value):
        ElementTree.SubElement(dict_elem, "key").text = key
        ElementTree.SubElement(dict_elem, "string").text = value

    def add_bool(key, val: bool):
        ElementTree.SubElement(dict_elem, "key").text = key
        ElementTree.SubElement(dict_elem, "true" if val else "false")

    def add_int(key, val: int):
        ElementTree.SubElement(dict_elem, "key").text = key
        ElementTree.SubElement(dict_elem, "integer").text = str(val)

    def add_env(env_vars: dict[str, str]):
        ElementTree.SubElement(dict_elem, "key").text = "EnvironmentVariables"
        env_dict = ElementTree.SubElement(dict_elem, "dict")
        for k, v in env_vars.items():
            ElementTree.SubElement(env_dict, "key").text = k
            ElementTree.SubElement(env_dict, "string").text = v

    add_kv("Label", f"local.borgadm.{task}")

    # EnvironmentVariables
    if env:
        add_env(env)

    # ProgramArguments
    ElementTree.SubElement(dict_elem, "key").text = "ProgramArguments"
    arr = ElementTree.SubElement(dict_elem, "array")
    for part in args:
        ElementTree.SubElement(arr, "string").text = part

    # Log paths
    add_kv("StandardOutPath", log_path)
    add_kv("StandardErrorPath", log_path)

    add_int("StartInterval", interval)
    add_kv("ProcessType", "Interactive")
    add_bool("LowPriorityIO", False)
    add_int("Nice", -5)

    return plist


def create_plist_file(element: ElementTree.Element, path: Path) -> None:
    rough_string = ElementTree.tostring(element, encoding="utf-8")
    reparsed = minidom.parseString(rough_string)
    # Get pretty-printed XML and skip the XML declaration line added by minidom
    pretty_xml = reparsed.toprettyxml(indent="    ")
    xml_lines = pretty_xml.split("\n")
    if xml_lines[0].startswith("<?xml"):
        xml_lines = xml_lines[1:]
    fd = os.open(path, os.O_WRONLY | os.O_CREAT | os.O_TRUNC, 0o644)
    with os.fdopen(fd, "w") as f:
        f.write('<?xml version="1.0" encoding="UTF-8"?>\n')
        f.write('<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN"\n')
        f.write(' "http://www.apple.com/DTDs/PropertyList-1.0.dtd">\n')
        f.write("\n".join(xml_lines))


# -----------------------------------------------------------------------------
# Subcommands
# -----------------------------------------------------------------------------
def do_automate(action: str) -> None:
    if platform.system() != "Darwin":
        logger.error("automate subcommand is only supported on osx")
        sys.exit(1)

    if not shutil.which("plutil"):
        logger.error("plutil not found in PATH")
        sys.exit(1)

    if not shutil.which("launchctl"):
        logger.error("launchctl not found in PATH")
        sys.exit(1)

    # Use the wrapper app for Full Disk Access support on macOS
    repo_dir = Path(__file__).parent.parent
    wrapper_app = repo_dir / "Applications/BorgAdm.app/Contents/MacOS/BorgAdm"
    if wrapper_app.exists():
        script_path: str = str(wrapper_app)
        logger.debug(f"Using wrapper app: {script_path}")
    else:
        script_path = os.path.abspath(__file__)
        logger.warning(
            f"Wrapper app not found at {wrapper_app}. "
            "Full Disk Access may not work. "
            "See Applications/BorgBackup.app in the repo."
        )
    launchd_env: dict[str, str] = {
        "PATH": os.path.expanduser("~/.local/bin")
        + ":/usr/bin:/bin:/usr/sbin:/sbin",
    }
    tasks: dict[str, dict[str, Any]] = {
        "create": {"args": [script_path, "create"], "interval": (60 * 60)},
        "check_age": {
            "args": [script_path, "check-age", "--enable-notifications"],
            "interval": (60 * 60 * 4),
        },
        "check_all": {
            "args": [script_path, "check-all", "--enable-notifications"],
            "interval": (60 * 60 * 24 * 7),
        },
    }
    task_name = "local.borgadm.{task}"
    task_dir: Path = Path(os.environ["HOME"]) / "Library" / "LaunchAgents"
    task2path: dict[str, Path] = {
        task: task_dir / (task_name.format(task=task) + ".plist")
        for task in tasks
    }
    cmd: list[str] = []

    if action == "status":
        launchctl_tasks = run_cmd(["launchctl", "list"]).stdout.splitlines()
        enabled_count = 0
        for task, path in task2path.items():
            tname = task_name.format(task=task)
            pattern = r"[ \t]" + re.escape(tname) + r"$"
            cpattern = re.compile(pattern)
            if any(cpattern.search(i) for i in launchctl_tasks):
                enabled_count += 1
                logger.info(f"{tname}: enabled ({path})")
            else:
                logger.info(f"{tname}: disabled")
        if enabled_count == len(task2path):
            logger.info("Automation is enabled")
        elif enabled_count == 0:
            logger.info("Automation is disabled")
        else:
            total = len(task2path)
            logger.info(
                f"Automation is partially enabled ({enabled_count}/{total})"
            )
        return

    # Always remove any existing configs for enable/disable
    for task, path in task2path.items():
        if not path.exists():
            continue
        logger.info(f"Removing {task} launchd config: {str(path)}")
        cmd = ["launchctl", "unload", str(path)]
        run_cmd(cmd, errok=True)
        path.unlink()

    if action == "disable":
        return

    log_dir = Path(os.environ["HOME"]) / "Library" / "Logs" / "borgadm"
    log_dir.mkdir(parents=True, exist_ok=True)
    logger.debug(f"Log directory: {log_dir}")

    for task, cfg in tasks.items():
        path = task2path[task]
        log_path = str(log_dir / f"{task}.log")
        logger.info(f"Creating {task} launchd config: {str(path)}")
        pe = create_plist_element(
            task, cfg["args"], cfg["interval"], launchd_env, log_path
        )
        create_plist_file(pe, path)
        cmd = ["plutil", str(path)]
        run_cmd(cmd)
        cmd = ["launchctl", "load", str(path)]
        run_cmd(cmd)

    # Print Full Disk Access instructions if using wrapper app
    if wrapper_app.exists():
        logger.info("")
        logger.info("=" * 60)
        logger.info("IMPORTANT: Grant Full Disk Access to enable backups")
        logger.info("=" * 60)
        logger.info("")
        logger.info("1. Run this command to open System Settings:")
        logger.info("")
        logger.info(
            '   open "x-apple.systempreferences:'
            'com.apple.settings.PrivacySecurity.extension?Privacy_AllFiles"'
        )
        logger.info("")
        logger.info("2. Click the '+' button")
        logger.info(
            f"3. Navigate to and add: {wrapper_app.parent.parent.parent}"
        )
        logger.info("4. Toggle the switch ON for BorgAdm")
        logger.info("")


def do_break_lock() -> None:
    cfg = get_cfg()

    logger.info(f"Breaking repo lock: {cfg.BORG_REPO}")
    run_cmd(
        borg_cmd()
        + [
            "break-lock",
            cfg.BORG_REPO,
        ]
    )


def do_check_age() -> None:
    cfg = get_cfg()
    seconds = cfg.CHECK_AGE_SECONDS
    backups = list_backups(latest=True)
    if not backups:
        logger.error("No full backups found.")
        sys.exit(2)
    assert len(backups.keys()) == 1, backups
    latest = list(backups.keys())[0]

    ts = datetime.strptime(latest, "%Y%m%d_%H%M%S")
    age = int((datetime.now() - ts).total_seconds())
    if age > seconds:
        logger.error(
            "Latest full backup (%s) is too old: %ss; limit is: %ss",
            latest,
            age,
            seconds,
        )
        sys.exit(3)

    logger.info(
        "Latest full backup (%s) age is: %s seconds; limit is: %s seconds",
        latest,
        age,
        seconds,
    )


def do_check_all(progress: bool) -> None:
    do_check_age()
    do_check_repo(progress)
    do_check_archives(progress)


def do_check_archives(progress: bool) -> None:
    backups = list_backups(latest=True)
    if not backups:
        logger.error("No full backups found.")
        sys.exit(1)
    assert len(backups.keys()) == 1
    archives = list(backups.values())[0]
    for archive in archives:
        logger.info(f"Checking archive metadata: {archive}")
        args = ["check", *(["--progress"] if progress else []), archive]
        run_cmd(borg_cmd() + args, allow_output=progress)


def do_check_repo(progress: bool) -> None:
    cfg = get_cfg()
    logger.info("Checking repo metadata.")
    args = ["check", *(["--progress"] if progress else []), cfg.BORG_REPO]
    run_cmd(borg_cmd() + args, allow_output=progress)


def do_compact(progress: bool) -> None:
    cfg = get_cfg()
    logger.info(f"Compacting repo: {cfg.BORG_REPO}")
    args = ["compact", *(["--progress"] if progress else []), cfg.BORG_REPO]
    run_cmd(borg_cmd() + args, allow_output=progress)


def do_create(dry_run: bool, progress: bool, no_prune: bool) -> None:
    cfg = get_cfg()

    def backup_set_tests(set_name: str) -> None:
        error: bool = False
        dirs, files = backup_set_paths(set_name)
        for d in dirs:
            full_path = cfg.BACKUP_ROOT / d
            if not os.path.isdir(full_path):
                logger.error(
                    f"Missing backup dir (set: '{set_name}'): {full_path}"
                )
                error = True
            elif not os.listdir(full_path):
                logger.error(
                    f"Empty backup dir (set: '{set_name}'): {full_path}"
                )
                error = True
        for f in files:
            full_path = cfg.BACKUP_ROOT / f
            if os.path.isdir(full_path):
                logger.error(
                    "Missing trailing slash for backup directory in config "
                    "(set: '%s'): %s",
                    set_name,
                    full_path,
                )
                error = True
            if not os.path.exists(full_path):
                logger.error(
                    f"Missing backup path (set: '{set_name}'): {full_path}"
                )
                error = True
        if error:
            sys.exit(1)

    # Run all tests before backups to fail-fast in case of errors
    for set_name in cfg.BACKUP_SETS:
        backup_set_tests(set_name)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    for set_name, set_cfg in cfg.BACKUP_SETS.items():
        # Backups can take a while, so re-run tests before each backup
        backup_set_tests(set_name)
        for path in cfg.BACKUP_MOUNTS:
            full_path = cfg.BACKUP_ROOT / path
            if not is_mountpoint(full_path):
                logger.error(f"Missing backup mount: {full_path}")
                sys.exit(1)
        create_args = set_cfg.get("create_options", [])
        create_args += ["--progress"] if progress else []
        create_args += ["--dry-run"] if dry_run else ["--stats"]
        excludes = list(set_cfg.get("excludes", []))
        if platform.system() == "Darwin":
            backup_paths = [Path(p.rstrip("/")) for p in set_cfg["paths"]]

            def is_under_backup_path(exclude: str) -> bool:
                exc_path = Path(exclude)
                return any(exc_path.is_relative_to(bp) for bp in backup_paths)

            tcc_excludes = [
                e
                for e in _MACOS_TCC_EXCLUDES
                if e not in excludes and is_under_backup_path(e)
            ]
            if tcc_excludes:
                logger.debug(
                    f"Auto-excluding {len(tcc_excludes)} macOS TCC-protected "
                    f"paths for set '{set_name}'"
                )
                excludes.extend(tcc_excludes)
        for e in excludes:
            create_args += ["--exclude", e]
        archive_name = f"{cfg.BACKUP_NAME}-{set_name}-{timestamp}"
        archive = f"{cfg.BORG_REPO}::{archive_name}"
        dry = " (dry-run)" if dry_run else ""
        logger.info(f"Create archive, start: {archive}{dry})")
        cmd = (
            borg_cmd()
            + ["create"]
            + create_args
            + [archive]
            + [os.path.expanduser(p) for p in set_cfg["paths"]]
        )
        run_cmd(cmd, cwd=cfg.BACKUP_ROOT, allow_output=progress)
        logger.info(f"Create archive, finish: {archive}{dry})")

    if not no_prune:
        do_prune(dry_run=dry_run, progress=progress)


def do_extract(
    target_dir: Path,
    patterns: list[str],
    dry_run: bool,
    delete: bool,
    progress: bool,
) -> None:
    backups = list_backups(latest=True)
    if not backups:
        logger.error("No full backups found.")
        sys.exit(1)
    assert len(backups.keys()) == 1
    archives = list(backups.values())[0]

    target_dir = target_dir.resolve()
    if not os.path.isdir(target_dir):
        logger.error(f"Invalid extraction dirctory: {target_dir}")
        sys.exit(1)

    logger.info(f"Extract target dirctory: {target_dir}")

    cleanup_paths: set[Path] = set()
    if delete:
        archive_paths: set[Path] = set()
        for archive in archives:
            logger.info(f"Getting archive contents: {archive}")
            cmd = borg_cmd() + ["list", "--format", "{path}{NL}"]
            for pattern in patterns:
                cmd += ["--pattern", pattern]
            cmd.append(archive)
            for i in run_cmd(cmd).stdout.splitlines():
                archive_paths.add(Path(i))

        logger.info(f"Getting target-path contents: {target_dir}")
        local_paths: set[Path] = set()
        for walk_root, walk_dirs, walk_files in os.walk(target_dir):
            root_path = Path(walk_root)
            for name in walk_files + walk_dirs:
                full_path = root_path / name
                rel_path = full_path.relative_to(target_dir)
                local_paths.add(rel_path)

        logger.info(
            "Filtering local contents against backup paths and archive contents"
        )
        bs_dirs, bs_files = backup_set_paths()

        def backup_set_filter(
            path: Path, set_name: Optional[str] = None
        ) -> bool:
            if path in bs_files:
                return True
            for d in bs_dirs:
                if path.is_relative_to(d):
                    return True
            return False

        local_paths = set(filter(backup_set_filter, local_paths))
        extras = sorted(
            local_paths - archive_paths, key=lambda x: len(x.parts)
        )  # shallowest first
        for path in extras:
            if not any(path.relative_to(p) for p in cleanup_paths):
                cleanup_paths.add(path)

        logger.info(
            f"Found {len(cleanup_paths)} paths to delete after extraction."
        )

    tdir_dirname = target_dir.parent
    tdir_basename = target_dir.name
    lockfile = tdir_dirname / f".{BASENAME}_{tdir_basename}.lock"
    with open(lockfile, "w") as lf:
        try:
            fcntl.flock(lf, fcntl.LOCK_EX | fcntl.LOCK_NB)
        except BlockingIOError:
            logger.error(
                "Another extract process is already running for this path."
            )
            sys.exit(1)

        for archive in archives:
            logger.info(f"Extracting: {archive} to {target_dir}")

            cmd = borg_cmd() + ["extract"]
            cmd += ["--progress"] if progress else []
            if dry_run:
                cmd.append("--dry-run")
            for pattern in patterns:
                cmd += ["--pattern", pattern]
            cmd.append(archive)
            run_cmd(cmd, cwd=target_dir, allow_output=progress)

    if cleanup_paths:
        delete_paths(target_dir, cleanup_paths, dry_run=dry_run)


def do_list(
    latest: bool,
    full_names: bool,
    include_partial: bool,
    only_partial: bool,
    keep_tags: bool,
) -> None:
    full_backups: dict[str, list[str]] = {}
    partial_backups: dict[str, list[str]] = {}
    if not only_partial:
        full_backups = list_backups(latest=latest)
    if include_partial or only_partial:
        partial_backups = list_backups(latest=latest, partial=True)
    backups = full_backups | partial_backups

    ts_keep: OrderedDict[str, str] = collections.OrderedDict()
    if keep_tags:
        ts_keep = ts_to_keep(set(full_backups.keys()))

    def get_keep_tag(ts) -> str:
        if not keep_tags:
            return ""
        if ts not in ts_keep:
            tag = "prune"
        else:
            tag = ts_keep[ts]
        return f" ({tag})"

    for ts, archives in backups.items():
        keep_tag = get_keep_tag(ts)
        if full_names:
            for archive in archives:
                logger.info(f"{archive}{keep_tag}")
        else:
            logger.info(f"{ts}{keep_tag}")


def do_prune(dry_run: bool, progress: bool) -> None:
    cfg = get_cfg()
    partial_archives = [
        a.removeprefix(f"{cfg.BORG_REPO}::")
        for archives in list_backups(partial=True).values()
        for a in archives
    ]
    if partial_archives:
        logger.info(f"Pruning partial archives: {partial_archives}")
        cmd = borg_cmd() + ["delete"]
        cmd += ["--progress"] if progress else []
        if dry_run:
            cmd.append("--dry-run")
        else:
            cmd.append("--stats")
        cmd.append(cfg.BORG_REPO)
        cmd += partial_archives
        run_cmd(cmd, allow_output=progress)

    archives = list_backups()
    ts_keep = ts_to_keep(set(archives.keys()))
    ts_prune = archives.keys() - ts_keep.keys()
    prune_archives = [
        a.removeprefix(f"{cfg.BORG_REPO}::")
        for ts in ts_prune
        for a in archives[ts]
    ]
    logger.info(f"Keeping {len(ts_keep)} and pruning {len(ts_prune)} backups.")
    for k, v in ts_keep.items():
        logger.info(f"Keeping backup: {k} ({v})")
    pruning = [f"{i}" for i in sorted(ts_prune, reverse=True)]
    logger.info(f"Pruning backups: {', '.join(pruning)}")
    if prune_archives:
        cmd = borg_cmd() + ["delete"]
        cmd += ["--progress"] if progress else []
        if dry_run:
            cmd.append("--dry-run")
        else:
            cmd.append("--stats")
        cmd.append(cfg.BORG_REPO)
        cmd += prune_archives
        run_cmd(cmd, allow_output=progress)

    do_compact(progress)


def do_rsync(
    target_dir: Path, dry_run: bool, delete: bool, progress: bool
) -> None:
    if platform.system() != "Linux":
        logger.error("Rsync subcommand is only supported on linux")
        sys.exit(1)

    if not check_sudo():
        logger.error(
            "Rsync subcommand requires non-interractive sudo support "
            "with the ability to set environment variables."
        )
        sys.exit(1)

    backups = list_backups(latest=True)
    if not backups:
        logger.error("No full backups found.")
        sys.exit(1)
    assert len(backups.keys()) == 1
    ts = list(backups.keys())[0]
    archives = list(backups.values())[0]

    target_dir = target_dir.resolve()
    if not os.path.isdir(target_dir):
        logger.error(f"Invalid extraction dirctory: {target_dir}")
        sys.exit(1)

    logger.info(f"Rsync target dirctory: {target_dir}")
    tdir_dirname = target_dir.parent
    tdir_basename = target_dir.name
    lockfile = tdir_dirname / f".{BASENAME}_{tdir_basename}.lock"
    tmpdir_prefix = f".{BASENAME}_{tdir_basename}.tmp."
    with (
        open(lockfile, "w") as lf,
        tempfile.TemporaryDirectory(
            dir=tdir_dirname,
            prefix=tmpdir_prefix,
        ) as tdir,
    ):
        tmpdir = Path(tdir)
        try:
            fcntl.flock(lf, fcntl.LOCK_EX | fcntl.LOCK_NB)
        except BlockingIOError:
            logger.error(
                "Another extract process is already running for this path."
            )
            sys.exit(1)

        cleanup_mounts: list[Path] = []
        cleanup_dirs: list[Path] = []
        try:
            archive_mounts: list[Path] = []
            for archive in archives:
                archive_name = archive.split("::", 1)[1]
                mount_dir = tmpdir / archive_name

                os.makedirs(mount_dir)
                cmd = borg_cmd() + [
                    "mount",
                    "-o",
                    "allow_other",
                    "--bypass-lock",
                    archive,
                    mount_dir.as_posix(),
                ]
                logger.info(f"Mounting {archive} to: {mount_dir}")
                run_cmd(cmd, sudo=True)
                cleanup_mounts.append(mount_dir)
                archive_mounts.append(mount_dir)

                if not os.listdir(mount_dir):
                    logger.error(f"Empty archive mount: {archive}")
                    sys.exit(1)

            lowerdir = ":".join([i.as_posix() for i in archive_mounts])
            upperdir = tmpdir / "upperdir"
            workdir = tmpdir / "workdir"
            merged = tmpdir / "merged"
            os.makedirs(upperdir)
            os.makedirs(workdir)
            os.makedirs(merged)
            cmd = [
                "mount",
                "-t",
                "overlay",
                "overlay",
                "-o",
                f"lowerdir={lowerdir},upperdir={upperdir},workdir={workdir}",
                merged.as_posix(),
            ]
            logger.info(f"Merging archives with overlayfs: {merged}")
            run_cmd(cmd, sudo=True)
            cleanup_mounts.append(merged)
            cleanup_dirs.append(workdir)
            if not os.listdir(merged):
                logger.error(f"Empty overlayfs mount: {merged}")
                sys.exit(1)

            logger.info(f"Starting rsync to: {target_dir}")
            cmd = [
                "rsync",
                "-a",
                "--stats",
                "--human-readable",
            ]
            cmd += ["--progress"] if progress else []
            cmd += ["--delete"] if delete else []
            cmd += ["--dry-run"] if dry_run else []
            cmd += [merged.as_posix() + "/", target_dir.as_posix() + "/"]
            run_cmd(cmd, allow_output=progress, sudo=True)

            # Record the version we extraced
            (tdir_dirname / f".{BASENAME}_{tdir_basename}.ts").write_text(
                f"{ts}"
            )

            logger.info(f"Finished rsync to: {target_dir}")
        finally:
            # Cleanup root owned resources
            for m in cleanup_mounts:
                run_cmd(["umount", "-f", m.as_posix()], sudo=True, errok=True)
            for d in cleanup_dirs:
                run_cmd(["rm", "-rf", d.as_posix()], sudo=True, errok=True)


def do_environment() -> None:
    cfg = get_cfg()
    logger.info(f"ssh-add -q {cfg.BORG_SSHKEY_FILE}")
    logger.info(f"export BORG_PASSPHRASE=$(cat {cfg.BORG_PASSPHRASE_FILE})")
    logger.info(f"export BORG_REPO={cfg.BORG_REPO}")
    if cfg.BORG_REMOTE_PATH:
        logger.info(f"export BORG_REMOTE_PATH={cfg.BORG_REMOTE_PATH}")


def do_self_test(verbose: bool = False, coverage: bool = False) -> None:
    """Run tests using pytest via uvx."""
    repo_root = Path(__file__).parent.parent
    tests_dir = repo_root / "tests"
    bin_dir = repo_root / "bin"

    cmd = ["uvx", "--with", "pytest"]
    if coverage:
        cmd.extend(["--with", "pytest-cov"])

    cmd.extend(
        ["pytest", "-p", "no:cacheprovider", str(tests_dir / "test_borgadm.py")]
    )

    if verbose:
        cmd.append("-v")
    if coverage:
        htmlcov_dir = Path(tempfile.gettempdir()) / "borgadm_htmlcov"
        cmd.extend(
            [
                "--cov=borgadm",
                "--cov-report=term-missing",
                f"--cov-report=html:{htmlcov_dir}",
            ]
        )

    env = os.environ.copy()
    env["PYTHONDONTWRITEBYTECODE"] = "1"
    env["PYTHONPATH"] = str(bin_dir)
    if coverage:
        env["COVERAGE_FILE"] = str(
            Path(tempfile.gettempdir()) / "borgadm.coverage"
        )

    result = subprocess.run(cmd, env=env, cwd=repo_root)
    sys.exit(result.returncode)


# -----------------------------------------------------------------------------
# Argument Parsing
# -----------------------------------------------------------------------------
def args_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Borg backup manager",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Always set formatter_class so that we display default values.
    def add_parser(name: str, **kwargs) -> argparse.ArgumentParser:
        kwargs["formatter_class"] = argparse.ArgumentDefaultsHelpFormatter
        return subparsers.add_parser(name, **kwargs)

    def add_delete(parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "--delete",
            action="store_true",
            help="Delete files in destination not in backup",
        )

    def add_dry_run(parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "--dry-run", action="store_true", help="don't make any changes"
        )

    def add_progress(parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "--progress", action="store_true", help="show progress information"
        )

    def add_prune_args(parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "--keep-hourly", type=int, help="Number of hourly backups to keep"
        )
        parser.add_argument(
            "--keep-daily", type=int, help="Number of daily backups to keep"
        )
        parser.add_argument(
            "--keep-weekly", type=int, help="Number of weekly backups to keep"
        )
        parser.add_argument(
            "--keep-monthly", type=int, help="Number of monthly backups to keep"
        )
        parser.add_argument(
            "--keep-yearly", type=int, help="Number of yearly backups to keep"
        )

    def add_seconds(parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "seconds",
            type=int,
            nargs="?",
            help="Maximum backup age, in seconds",
        )

    def add_target_dir(parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "target_dir", type=Path, help="Target directly for extracted backup"
        )

    automate = add_parser(
        "automate",
        help="Enable/Disable automated backups and checks",
    )
    automate_subparsers = automate.add_subparsers(dest="action", required=True)
    automate_subparsers.add_parser(
        "enable",
        help="Enable automated backup creation and checks",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    automate_subparsers.add_parser(
        "disable",
        help="Disable automated backup creation and checks",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    automate_subparsers.add_parser(
        "status",
        help="Show automation status and launchd file paths",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    add_parser("break-lock", help="Break repo lock")

    check_age = add_parser("check-age", help="Check full backup age")
    add_seconds(check_age)

    check_all = add_parser("check-all", help="Run all checks")
    add_progress(check_all)
    add_seconds(check_all)

    check_repo = add_parser("check-repo", help="Check repo metadata")
    add_progress(check_repo)

    check_archives = add_parser("check-archives", help="Check archive metadata")
    add_progress(check_archives)

    compact = add_parser("compact", help="Check full backup metadata")
    add_progress(compact)

    create = add_parser("create", help="Create full backup")
    create.add_argument(
        "--no-prune", action="store_true", help="Skip backup pruning"
    )
    add_dry_run(create)
    add_progress(create)
    add_prune_args(create)

    extract = add_parser("extract", help="Extract latest full backup")
    add_target_dir(extract)
    extract.add_argument(
        "patterns",
        nargs="*",
        help=(
            "include/exclude paths matching PATTERN, "
            "see 'borg help patterns' for details"
        ),
    )
    add_delete(extract)
    add_dry_run(extract)
    add_progress(extract)

    list_cmd = add_parser("list", help="List backups")
    list_cmd.add_argument(
        "--latest",
        action="store_true",
        help="Only list latest complete backup set",
    )
    list_cmd.add_argument(
        "--full-names",
        action="store_true",
        help="List full names of backups (instead of just timestamps)",
    )
    list_cmd.add_argument(
        "--keep-tags",
        action="store_true",
        help=(
            "Include keep tags (indicating what would be kept and "
            "deleted during a prune)"
        ),
    )
    add_prune_args(list_cmd)
    group = list_cmd.add_mutually_exclusive_group()
    group.add_argument(
        "--include-partial", action="store_true", help="Include partial backups"
    )
    group.add_argument(
        "--only-partial", action="store_true", help="Only list partial"
    )

    prune = add_parser("prune", help="Prune partial and old backups")
    add_dry_run(prune)
    add_progress(prune)
    add_prune_args(prune)

    rsync = add_parser("rsync", help="Rsync latest archive")
    add_target_dir(rsync)
    add_delete(rsync)
    add_dry_run(rsync)
    add_progress(rsync)

    add_parser(
        "environment", help="Display environment settings to run borg cli"
    )

    self_test = add_parser("self-test", help="Run tests")
    self_test.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="Verbose test output",
    )
    self_test.add_argument(
        "--coverage",
        action="store_true",
        help="Run with coverage report",
    )

    for name, subparser in subparsers.choices.items():
        # self-test doesn't need config or notification args
        if name == "self-test":
            continue
        subparser.add_argument(
            "--config", type=str, default=CONFIG, help="borgadm config file"
        )
        subparser.add_argument(
            "--enable-notifications",
            action="store_true",
            help=(
                "Enable system notifications for command failures "
                "(only supported on osx)"
            ),
        )
        subparser.add_argument(
            "--verbose", action="store_true", help="Enable verbose output"
        )

    return parser


# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------
def main() -> None:
    args = args_parser().parse_args()
    args_dict = vars(args).copy()
    command = args_dict.pop("command")

    # Handle self-test early - it doesn't need config or borg environment
    if command == "self-test":
        do_self_test(**args_dict)
        return

    config = args_dict.pop("config")
    verbose = args_dict.pop("verbose")
    global _enable_notifications
    _enable_notifications = args_dict.pop("enable_notifications")

    initialize_logger(LOGFILE, verbose)

    invocation = " ".join(shlex.quote(arg) for arg in sys.argv)
    logger.debug(f"Invocation: {invocation}")
    atexit.register(report_runtime)

    global CFG
    CFG = Config(config, args_dict)

    initialize_borg_environment()

    if command not in ["break-lock", "check-repo", "compact", "environment"]:
        logger.debug(f"BACKUP_SETS={CFG.BACKUP_SETS}")

    command_callbacks: dict[str, Callable[..., Any]] = {
        "automate": do_automate,
        "break-lock": do_break_lock,
        "check-age": do_check_age,
        "check-all": do_check_all,
        "check-archives": do_check_archives,
        "check-repo": do_check_repo,
        "compact": do_compact,
        "create": do_create,
        "environment": do_environment,
        "extract": do_extract,
        "list": do_list,
        "prune": do_prune,
        "rsync": do_rsync,
    }
    try:
        command_callbacks[command](**args_dict)
    except SystemExit as e:
        if e.code != 0:
            osascript_notify()
        raise


if __name__ == "__main__":
    main()
